<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html>
<head>

<title>VMware ESXi 6.0 Update 2 Release Notes</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>

<body lang="EN-US" xml:lang="EN-US">

<div id="ibg_styles"></div>  <!-- import background styles -->
<div id="container">
<div id="ivmware_logo"></div> <!-- import vmware logo -->
<div id="content-container"><div id="content">

<table cellpadding="0" cellspacing="0" id="main-table">
  <tr>
	<td id="main-body">
	
<!-- ///*** start of content area ***/// -->




<h1>VMware ESXi 6.0 Update 2 Release Notes</h1>

	         <table width="100%"  border="0" cellpadding="10" cellspacing="0">
       	<tr>
      	<td class="features">

<p>Updated on: 04 MAY 2016</p>
              
		  <P><strong>ESXi 6.0 Update 2 | 15 MAR 2016 | ISO Build 3620759</strong></P>
		  
<p>Check for additions and updates to these release notes.</p>
		  </td></tr>
			</table>
		
<H2>What's in the Release Notes</H2>
<p>The release notes cover the following topics:</p>
<ul>
<li><a href="#whatsnew">What's New</a></li>
<li><a href="#earlierrelease">Earlier Releases of ESXi 6.0</a></li>
<li><a href="#i18n">Internationalization</a></li>
<li><a href="#compatibility">Compatibility</a></li>
<li><a href="#installupgrade">Installation and Upgrades for This Release</a></li>
<li><a href="#featureplatfornotice">Product Support Notices</a></li>
<li><a href="#patchesinrelease">Patches Contained in This Release</a></li>
<li><a href="#resolvedissues">Resolved Issues</a></li>
<li><a href="#knownissues">Known Issues</a></li>
</ul>

<a name="whatsnew"></a><h2>What's New</h2>

<UL>
<li><b>High Ethernet Link Speed:</b> ESXi 6.0 Update 2 supports 25 G and 50 G ethernet link speeds.<br>
  <br></li>
<li><b>VMware Host Client:</b> The VMware Host Client is an HTML5 client that is used to connect to and manage single ESXi hosts. It can be used to perform administrative tasks to manage host resources, such as virtual machines, networking, and storage. The VMware Host Client can also be helpful to troubleshoot individual virtual machines or hosts when vCenter Server and the vSphere Web Client are unavailable. For more information about VMware Host Client, see <a href="http://pubs.vmware.com/Release_Notes/en/vsphere/60/vmware-host-client-10-release-notes.html">VMware Host Client 1.0 Release Notes</a>.<br><br></li>
<li><b>VMware Virtual SAN 6.2:</b> The new VMware Virtual SAN 6.2 is bundled with ESXi 6.0 Update 2. For more information about Virtual SAN, see <a href="http://pubs.vmware.com/Release_Notes/en/vsan/62/vmware-virtual-san-62-release-notes.html">VMware Virtual SAN 6.2 Release Notes</a>.<br><br>
Virtual SAN adds a new <tt>vsan</tt> vib in the ESXi image. The new vib results in a dependency being added to the <tt>esx-base</tt> vib requiring the <tt>vsan</tt> vib to be installed on the new 6.0 Update 2 host. For more information, see <a href="http://kb.vmware.com/kb/2144595">Knowledge Base article 2144595</a>.<br>
<br></li>
<li><strong>vSphere APIs for I/O Filtering (VAIO) Enhancement:</strong><br><br>
<ul>
<li>ESXi 6.0 Update 2 supports the IO Filter VASA Provider in a pure IPv6 environment. For additional information, see the <a href="#iofilteripv6">Resolved Issues</a> section.<br><br></li>
<li>ESXi 6.0 Update 2 supports the VMIOF versions 1.0 and 1.1. For additional information, see the <a href="#vmiof">Resolved Issues</a> section.<br><br></li></ul></li>
<!--<li><b>TLS Protocol Support:</b> ESXi now supports configuration of TLSv1, TLSv1.1, and TLSv1.2 protocols. For configuration details, see <a href="http://kb.vmware.com/kb/2144611">Knowledge Base article 2144611</a>.<br><br></li>-->
<li>ESXi 6.0 Update 2 addresses issues that have been documented in the <a href="#resolvedissues">Resolved Issues</a> section.</li>
</UL>



<a name="earlierrelease"></a><h2>Earlier Releases of ESXi 6.0</h2>
<p>Features and known issues of ESXi 6.0 are described in the release notes for each release. Release notes for earlier releases of ESXi 6.0, are:</p>
<ul>
<li><a href="http://pubs.vmware.com/Release_Notes/en/vsphere/60/vsphere-esxi-60u1b-release-notes.html" target="_blank">VMware 
ESXi 6.0 Update 1b Release Notes</a></li>
<li><a href="http://pubs.vmware.com/Release_Notes/en/vsphere/60/vsphere-esxi-60u1a-release-notes.html" target="_blank">VMware 
ESXi 6.0 Update 1a Release Notes</a></li>
<li><a href="https://www.vmware.com/support/vsphere6/doc/vsphere-esxi-vcenter-server-60-release-notes.html" target="_blank">VMware 
vSphere 6.0 Release Notes</a></li>
</ul>



<a name="i18n"></a><h2>Internationalization</h2>
<p>VMware ESXi 6.0 is available in the following languages:</p>
<ul>
<li>English</li>
<li>French</li>
<li>German</li>
<li>Japanese</li>
<li>Korean</li>
<li>Simplified Chinese</li>
<li>Spanish</li>
<li>Traditional Chinese</li>
</ul>

<p>Components of VMware vSphere 6.0, including vCenter Server, ESXi, the vSphere Web Client, and the vSphere Client do not 
accept non-ASCII input.</p>

<a name="compatibility"></a><h2>Compatibility</h2>

<a name="compatibility"></a><h3>ESXi, vCenter Server, and vSphere Web Client Version Compatibility</h3>

<p>The <a href="http://partnerweb.vmware.com/comp_guide/sim/interop_matrix.php" target="_blank">VMware 
Product Interoperability Matrix</a> provides details about the compatibility of current and earlier
versions of VMware vSphere components, including ESXi, VMware vCenter Server, the vSphere Web Client, 
and optional VMware products. Check the <a href="http://partnerweb.vmware.com/comp_guide/sim/interop_matrix.php" target="_blank">VMware 
Product Interoperability Matrix</a> also for information about supported management 
and backup agents before you install ESXi or vCenter Server.
<p>

The vSphere Web Client is packaged with the vCenter Server.  You can install the vSphere Client from the VMware vCenter
autorun menu that is part of the modules ISO file.</p>


<a name="hardware_compatibility"></a><h3>Hardware Compatibility for ESXi</h3>

<p>To view a list of processors, storage devices, SAN arrays, and I/O devices that are compatible with 
vSphere 6.0, use the ESXi 6.0  information in the 
<a href="http://www.vmware.com/resources/compatibility/search.php" target="_blank"> VMware Compatibility 
Guide</a>.</p> 


<a name="device_compatibility"></a><h3>Device Compatibility for ESXi</h3>

<p>To determine which devices are compatible with ESXi 6.0, use the ESXi 6.0 information in the
<a href="http://www.vmware.com/resources/compatibility/search.php" target="_blank"> VMware Compatibility 
Guide</a>.</p>

<p>Some devices are deprecated and no longer supported on ESXi 6.0. During the upgrade process, the device driver is 
installed on the ESXi 6.0 host. The device driver might still function on ESXi 6.0, but the device is not supported on ESXi 6.0. For a 
list of devices that are deprecated and no longer supported on ESXi 6.0, see <a href="http://kb.vmware.com/kb/2087970" target="_blank">KB 2087970</a>.</p> 


<a name="switch_compatibility"></a><h3>Third-Party Switch Compatibility for ESXi</h3>

<p>VMware now supports Cisco Nexus 1000V with vSphere 6.0. vSphere requires a minimum NX-OS release of 5.2(1)SV3(1.4).  
For more information about Cisco Nexus 1000V, see the <a href="http://www.cisco.com/c/en/us/td/docs/switches/datacenter/nexus1000/sw/5_x/release_notes/b_Cisco_NIKV_VMware_521SV314_ReleaseNotes.html" target="_blank"> 
Cisco Release Notes</a>.
As in previous vSphere releases, Ciscso Nexus 1000V AVS mode is not supported.</p>


<a name="gos_compatibility"></a><h3>Guest Operating System Compatibility for ESXi</h3>

<p>To determine which guest operating systems are compatible with vSphere 6.0, use the 
ESXi 6.0 information in the <a href="http://www.vmware.com/resources/compatibility/search.php" 
target="_blank"> VMware Compatibility Guide</a>.</p> 
</p>


<a name="vm_compatibility"></a><h3>Virtual Machine Compatibility for ESXi</h3>

<p>Virtual machines that are compatible with ESX 3.x and later (hardware version 4) are supported with 
ESXi 6.0. Virtual machines that are compatible with ESX 2.x and later (hardware version 3) are not
supported. To use such virtual machines on ESXi 6.0, upgrade the virtual machine compatibility. See the 
<a href="http://pubs.vmware.com/vsphere-60/topic/com.vmware.vsphere.upgrade.doc/GUID-18B7B4BB-C24A-49CD-AE76-13285157B29F.html
" target="_blank"> <em>vSphere Upgrade</em></a> documentation.</p>


<a name="installupgrade"></a><h2>Installation and Upgrades for This Release</h2>

<a name="installnotes"></a><h3>Installation Notes for This Release</h3>

<p>Read the 
<a href="http://pubs.vmware.com/vsphere-60/topic/com.vmware.vsphere.install.doc/GUID-7C9A1E23-7FCD-4295-9CB1-C932F2423C63.html
" target="_blank"> <em>vSphere Installation and Setup</em></a>
documentation for guidance about installing and 
configuring ESXi and vCenter Server.</p>

<p>Although the installations are straightforward, several subsequent configuration steps are essential. 
Read the following documentation:</p>
<ul>
<li><p>&quot;License Management and Reporting&quot; in the
<a href="http://pubs.vmware.com/vsphere-60/topic/com.vmware.vsphere.vcenterhost.doc/GUID-3B5AF2B1-C534-4426-B97A-D14019A8010F.html
" target="_blank"> <em>vCenter Server and Host Management</em></a> documentaton</p></li> 
<li><p>&quot;Networking&quot; in the
<a href="http://pubs.vmware.com/vsphere-60/topic/com.vmware.vsphere.networking.doc/GUID-35B40B0B-0C13-43B2-BC85-18C9C91BE2D4.html
" target="_blank"> <em>vSphere Networking</em></a> documentation</p></li>
<li><p>&quot;Security&quot; in the 
<a href="http://pubs.vmware.com/vsphere-60/topic/com.vmware.vsphere.security.doc/GUID-52188148-C579-4F6A-8335-CFBCE0DD2167.html
" target="_blank"> <em>vSphere Security</em></a>
documentation for information on firewall ports</p></li>
</ul>

<!--
<p><b>Note:</b> This release of vCenter Server 6.0 and vCenter Server Appliance 6.0 supports installation in a pure IPV4-only or IPV6-only 
environment.  vCenter Server 6.0 and vCenter Server Appliance 6.0 can also be installed in an All-in-one-only node.
-->
</p> 

<h3>vSphere 6.0 Recommended Deployment Models</h3>
<p>VMware recommends only two deployment models:</p>
<ul>
<li><p><strong>vCenter Server with embedded Platform Services Controller</strong>. This model is recommended if one or more standalone vCenter Server 
instances are required to be deployed in a data center. Replication between these vCenter Server with embedded Platform Services 
Controller models are not recommended.</p></li>
<li><p><strong>vCenter Server with external Platform Services Controller</strong>. This model is recommended only if multiple vCenter Server instances need 
to be linked or want to have reduced footprint of Platform Services Controller in the data center. Replication between these 
vCenter Server with external Platform Services Controller models are supported.</p></li>
</ul>
<p>Read the <i><a href="http://pubs.vmware.com/vsphere-60/topic/com.vmware.vsphere.install.doc/GUID-7C9A1E23-7FCD-4295-9CB1-C932F2423C63.html" target="_blank">vSphere Installation and Setup</a></i> 
documentation for guidance on installing and configuring vCenter Server. 
<p>Read the <a href="http://kb.vmware.com/kb/2109760" target="_blank"> Update sequence for vSphere 6.0 and its compatible VMware products</a> for the proper sequence in which vSphere components should be updated.</p>
</li></p>

<p>Also, read <a href="http://kb.vmware.com/kb/2108548" target="_blank">KB 2108548</a> 
for guidance on installing and configuring vCenter Server.</p>


<h3>vCenter Host OS Information</h3>
<p>Read the Knowledge Base article <a href="http://kb.vmware.com/kb/2091273" target="_blank">KB 2091273</a>.</p>
 

<h3>Backup and Restore for vCenter Server and the vCenter Server Appliance Deployments that Use an External Platform
Services Controller</h3>

<p>Although statements in the <i><a href="http://pubs.vmware.com/vsphere-60/topic/com.vmware.vsphere.install.doc/GUID-7C9A1E23-7FCD-4295-9CB1-C932F2423C63.html" target="_blank">vSphere 
Installation and Setup</a></i> documentation restrict you from attempting to backup and restore vCenter Server and vCenter
Server Appliance deployments that use an external Platform Services Controller, you can perform this task by following the
steps in <a href="http://kb.vmware.com/kb/2110294" target="_blank">KB 2110294</a>.
</p>
 
 
 
<h3>Migration from Embedded Platform Services Controller to External Platform Services Controller</h3>
<p>vCenter Server with embedded Platform Services Controller cannot be migrated automatically to vCenter Server with external 
Platform Services Controller. Testing of this migration utility is not complete.</p>
 
<p>Before installing vCenter Server, determine your desired deployment option. If more than one vCenter Servers are required for
replication setup, always deploy vCenter with external Platform Services Controller.</p>

<h3>Migrating Third-Party Solutions</h3>
<p>For information about upgrading with third-party customizations, see the 
<i><a href="http://pubs.vmware.com/vsphere-60/topic/com.vmware.vsphere.upgrade.doc/GUID-18B7B4BB-C24A-49CD-AE76-13285157B29F.html" target="_blank">vSphere Upgrade</a></i>
documentation. For information about using Image Builder to make a custom ISO, see the 
<i><a href="http://pubs.vmware.com/vsphere-60/topic/com.vmware.vsphere.install.doc/GUID-7C9A1E23-7FCD-4295-9CB1-C932F2423C63.html" target="_blank">vSphere Installation and Setup</a></i>
documentation.</p>


<h3>Upgrades and Installations Disallowed for Unsupported CPUs</h3>
<p>vSphere 6.0 supports only processors available after June (third quarter) 2006.  Comparing the processors supported by 
vSphere 5.x, vSphere 6.0 no longer supports the following processors:
<ul>
<li>AMD Opteron 12xx Series</li>
<li>AMD Opteron 22xx Series</li>
<li>AMD Operton 82xx Series</li>
</ul>
</p> 
<p>During an installation or upgrade, the installer checks the compatibility of the host CPU with vSphere 6.0. 
If your host hardware is not compatible, a purple screen appears with an incompatibility information message, 
and the vSphere 6.0 installation process stops. 
</p>

<h3>Upgrade Notes for This Release</h3>

<p>For instructions about upgrading vCenter Server and ESX/ESXi hosts, see the 
<a href="http://pubs.vmware.com/vsphere-60/topic/com.vmware.vsphere.upgrade.doc/GUID-18B7B4BB-C24A-49CD-AE76-13285157B29F.html
" target="_blank"><em>vSphere Upgrade</em></a> documentation.</p> 



<a name="opensource"></a><h2>Open Source Components for VMware vSphere 6.0</h2>

<p>The copyright statements and licenses applicable to the  open source software components distributed in 
vSphere 6.0 are available at <a href="http://www.vmware.com" target="_blank">http://www.vmware.com</a>. You need to log in to your
My VMware account.  Then, from the <strong>Downloads</strong>
menu, select <strong>vSphere</strong>. On the <strong>Open Source</strong> tab,
you can also download the source files for any GPL, LGPL, or other similar licenses that require  the source code or modifications to source code 
to be made available for the most recent available release of vSphere.</p>  



<a name="featureplatfornotice"></a><h2>Product Support Notices</h2>

<ul>
<li><p><strong>vCenter Server database</strong>. Oracle 11g and 12c as an external database for vCenter Server Appliance has been deprecated in 
the vSphere 6.0 release. VMware continues to support Oracle 11g and 12c as an external database in vSphere 6.0. VMware will drop support
for Orace 11g and 12c as an external database for vCenter Server Appliance in a furture major release.</p>
</li>
<li><p><strong>vSphere Web Client</strong>. The <strong>Storage Reports</strong> selection from an object's <strong>Monitor</strong> tab is no longer available 
in the vSphere 6.0 Web Client.</p></li>
<li><p><strong>vSphere Client</strong>. The <strong>Storage Views</strong> tab is no longer available in the 
vSphere 6.0 Client.</p></li>
</ul>


<h2><a name="patchesinrelease"></a>Patches Contained in this Release</h2>
<p>This release contains all bulletins for ESXi that were released earlier  to the release date of this product. See the VMware <a href="http://support.vmware.com/selfsupport/download/" target="_blank">Download Patches</a> page for more information about the individual bulletins. </p>
<p>Patch Release <a href="http://kb.vmware.com/kb/2142184" target="_blank">ESXi600-Update02</a> contains the following individual bulletins:</p>
<p><a href="http://kb.vmware.com/kb/2142185" target="_blank">ESXi600-201603201-UG: Updates esx-base, vsanhealth, vsan vib</a><br>
<a href="http://kb.vmware.com/kb/2142186" target="_blank">ESXi600-201603202-UG: Updates misc-drivers, xhci-xhci, others</a><br>
<a href="http://kb.vmware.com/kb/2142187" target="_blank">ESXi600-201603203-UG: Updates ESXi 6.0 esx-tboot vib</a><br>
<a href="http://kb.vmware.com/kb/2142188" target="_blank">ESXi600-201603204-UG-BG: Updates ESXi 6.0 tools-light vib</a><br>
<a href="http://kb.vmware.com/kb/2142189" target="_blank">ESXi600-201603205-UG-BG: Updates ESXi 6.0 esx-ui vib</a><br>
<a href="http://kb.vmware.com/kb/2143768 " target="_blank">ESXi600-201603206-UG-BG: Updates ESXi 6.0 nvme vib</a><br>
<a href="http://kb.vmware.com/kb/2143770 " target="_blank">ESXi600-201603207-UG-BG: Updates ESXi 6.0 net-vmxnet3 vib</a><br>
<a href="http://kb.vmware.com/kb/2143771 " target="_blank">ESXi600-201603208-UG-BG: Updates ESXi 6.0 sata-ahci vib</a><br>

  </p>
 <p>Patch Release <a href="http://kb.vmware.com/kb/2142184" target="_blank">ESXi600-Update02 (Security-only build)</a> contains the following individual bulletins:</p>
 <p><a href="http://kb.vmware.com/kb/2142192" target="_blank">ESXi600-201603101-SG: Updates ESXi 6.0 esx-base vib</a><br>
<a href="http://kb.vmware.com/kb/2142193" target="_blank">ESXi600-201603102-SG: Updates ESXi 6.0 tools-light vib</a><br></p>

<p>Patch Release <a href="http://kb.vmware.com/kb/2142184" target="_blank">ESXi600-Update02</a> contains the following image profiles:</p>
<p><a href="http://kb.vmware.com/kb/2142190" target="_blank">ESXi-6.0.0-20160302001-standard</a><br>
  <a href="http://kb.vmware.com/kb/2142191" target="_blank">ESXi-6.0.0-20160302001-no-tools</a></p>
  
  <p>Patch Release <a href="http://kb.vmware.com/kb/2142184" target="_blank">ESXi600-Update02 (Security-only build)</a> contains the following image profiles:</p>
<p><a href="http://kb.vmware.com/kb/2142194" target="_blank">ESXi-6.0.0-20160301001s-standard</a><br>
  <a href="http://kb.vmware.com/kb/2142195" target="_blank">ESXi-6.0.0-20160301001s-no-tools</a></p>

<p>For information on patch and update classification, see <a href="http://kb.vmware.com/kb/2014447" target="_blank">KB 2014447</a>.</p>


<h2><a name="resolvedissues" id="resolvedissues"></a>Resolved Issues</h2>
<p>The resolved issues are grouped as follows. </p> 
<ul>
<!--<li><a href="#radib_issues">Auto Deploy and Image Builder Issues</a></li>
<li><a href="#rbackupissues">Backup Issues</a></li>-->
<li><a href="#rcmiapiissues">CIM and API Issues</a></li>
<!--<li><a href="#rgosissues">Guest Operating System Issues</a></li>
<li><a href="#ri18nissues">Internationalization Issues</a></li> 
<li><a href="#rlicensingissues">Licensing Issues</a></li>-->
<li><a href="#rmiscissues">Miscellaneous Issues</a></li>
<li><a href="#rnetworkingissues">Networking Issues</a></li>
<li><a href="#rsecurityissues">Security Issues</a></li>
<li><a href="#rserverissues">Server Configuration Issues</a></li>
<li><a href="#rstorageissues">Storage Issues</a></li>
<!--<li><a href="#rhardwareissues">Supported Hardware Issues</a></li>-->
<li><a href="#rupgradeinstallissues">Upgrade and Installation Issues</a></li>
<!--<li><a href="#rupgradeissues">Upgrade Issues</a></li>-->
<li><a href="#rclientissues">vCenter Server, vSphere Web Client, and vSphere Client Issues</a></li>
<!--<li><a href="#rvcenterssocmissues">vCenter Single Sign-On and Certificate Management Issues</a></li>-->
<li><a href="#rvmissues">Virtual Machine Management Issues</a></li>
<!--<li><a href="#rvsanissues">Virtual SAN Issues</a></li>-->
<li><a href="#rvmotionstorage">vMotion and Storage vMotion Issues</a></li>
<!-- <li><a href="#rvmwarehaissues">VMware HA and Fault Tolerance Issues</a></li>-->
<li><a href="#rvmtoolsissues">VMware Tools Issues</a></li> 
<!--<li><a href="#rvicli_issues">vSphere Command-Line Interface Issues</a></li>-->
</ul>

<!--<b><a name="radib_issues"></a>Auto Deploy and Image Builder Issues</b>-->

<!--<b><a name="rbackupissues"></a>Backup Issues</b>-->
<br><br>
<b><a name="#rcimapiissues"></a>CIM and API Issues</b>


<ul>
  <li><a name="1551431"  id="1551431" ></a>
                     <!--Dev PR:1551431 Doc PR:1563000  -->
             <strong>Hardware monitoring might fail when you use third-party software ServerView RAID Manager</strong><br>
             You might experience hardware monitoring failure when you use third-party software ServerView RAID Manager. The <tt>sfcb-vmware_aux</tt> stops responding due to race condition.<br><br>
             This issue is resolved in this release.<br><br>
         </li>
</ul>

<ul><li><a name="1555228"  id="1555228" ></a>
                     <!--Dev PR:1555228 Doc PR:1578347  -->
             <strong>Hardware Status tab stops responding with an error message </strong><br>
             The Hardware Status tab might stop responding when FRU device is accessed on word and the <tt>read_cnt</tt> is greater than or equal to  1. An error message similar to the following is logged in the <tt>syslog.log</tt> file:<br><br>
<tt>Dropped response operation details -- nameSpace: root/cimv2, className: OMC_RawIpmiEntity, Type: 0\</tt><br><br>
             This issue is resolved in this release.
        <br><br> </li>
</ul>  

  
<!--<b><a name="#rgosissues"></a>Guest Operating System Issues</b>-->

<!--<b><a name="#ri18nissues"></a>Internationalization Issues</b>-->

<!--<b><a name="rlicensingissues"></a>Licensing Issues</b>-->

<b><a name="rmiscissues"></a>Miscellaneous Issues</b>

<ul><li><a name="1575130"  id="1575130" ></a>
                     <!--Dev PR:1575130 Doc PR:1578349  -->
             <strong>Hostd stops responding when esxcli commands are executed using PowerCLI</strong><br>
             Hostd might stop responding when you execute esxcli commands using PowerCLI resulting in memory leaks and memory consumption exceeding the hard limit. Error message similar to the following is logged in the <tt>hostd.log</tt> file:<br><br>
<tt>YYYY-MM-DDTHH:MM:SS.135Z [nnnnnnnn error 'UW Memory checker'] Current value 646016 exceeds hard limit 643993. Shutting down process.</tt><br><br>
             This issue is resolved in this release.
         </li>
</ul>

<ul><li><a name="1581300"  id="1581300" ></a>
                     <!--Dev PR:1581300 Doc PR:1602868 -->
             <strong>ESXi host experiences a purple diagnostic screen with multiple Correctable Machine Check Interrupt (CMCI) messages</strong><br><br>
The ESXi host might fail with a purple diagnostic screen due to an unresponsive CPU as a result of several CMCIs in the <tt>vmkernel.log</tt> file within a short time. Entries similar to the following are displayed on the purple diagnostic screen:<br><br>
<tt>PCPU &#60;N&#62;: no heartbeat (2/2 IPIs received)<br>
0xXXXXXXXXXXXX:[0xXXXXXXXXXXXX]MCEReapMCABanks@vmkernel#nover+0x195<br>
0xXXXXXXXXXXXX:[0xXXXXXXXXXXXX]MCEHandleCMCI@vmkernel#nover+0xb4<br>
0xXXXXXXXXXXXX:[0xXXXXXXXXXXXX]IRQ_DoInterrupt@vmkernel#nover+0x33e<br>
0xXXXXXXXXXXXX:[0xXXXXXXXXXXXX]IDT_IntrHandler@vmkernel#nover+0x12b
0xXXXXXXXXXXXX:[0xXXXXXXXXXXXX]gate_entry@vmkernel#nover+0x64<br>
0xXXXXXXXXXXXX:[0xXXXXXXXXXXXX]LFQueue_Dequeue@vmkernel#nover+0x59<br>
0xXXXXXXXXXXXX:[0xXXXXXXXXXXXX]MCEBottomHalf@vmkernel#nover+0x39<br>
0xXXXXXXXXXXXX:[0xXXXXXXXXXXXX]BH_DrainAndDisableInterrupts@vmkernel#nover+0xf3<br>
0xXXXXXXXXXXXX:[0xXXXXXXXXXXXX]VMMVMKCall_Call@vmkernel#nover+0x2c6</tt><br><br>

Entries similar to the following are logged in the <tt>vmkernel.log</tt> file:<br><br>
<tt>cpu1:33127)MCE: 1118: cpu1: MCA error detected via CMCI (Gbl status=0x0): Restart IP: invalid, Error IP: invalid, MCE in progress: no.</tt><br>
<tt>cpu1:33127)MCE: 231: cpu1: bank9: MCA recoverable error (CE): "Memory Controller Scrubbing Error on Channel 0."</tt><br>
<tt>cpu1:33127)MCE: 222: cpu1: bank9: status=0xXXXXXXXXXXXXXXXX: (VAL=1, OVFLW=0, UC=0, EN=0, PCC=0, S=0, AR=0), ECC=no, Addr:0xXXXXXXXXXXXXXXXX (valid), Misc:0x8c3589300 (valid)</tt><br><br>
This issue is resolved in this release.
            <br><br> </li>
</ul>



<b><a name="rnetworkingissues"></a>Networking Issues</b>

<ul><li><a name="1510290"  id="1510290" ></a>
                     <!--Dev PR:1510290 Doc PR:1608124  -->
             <strong>The vSISH statistics show huge packet drops</strong><br>
             The VSISH stats might show huge false positive packet drops due to integer overflow.<br><br>
This issue is resolved in this release.</li>
</ul>


<ul><li><a name="1505945"  id="1505945" ></a>
                     <!--Dev PR:1505945 Doc PR:1629144  -->
             <font color=#CC0000><strong>New </strong></font><strong>ESXi host with Netflow function enabled in VXLAN environment might fail with a purple diagnostic screen</strong><br>
             An ESXi host with Netflow function enabled in VXLAN environment might fail with a purple diagnostic screen with entries similar to:<br><br>
<tt>@BlueScreen: PANIC bora/vmkernel/main/dlmalloc.c:4923 - Usage error in dlmalloc<br>
PTEs:0xnnnnnnnn;0xnnnnnnnn;0x0;<br>
0xnnnnnnnn:[0xnnnnnnnn]PanicvPanicInt@vmkernel#nover+0x37e<br>
....<br>
....<br>
0xnnnnnnnn:[0xnnnnnnnn]Net_AcceptRxList@vmkernel#nover+0x115</tt><br><br>
This issue is resolved in this release.</li>
</ul>

<ul><li><a name="1550633"  id="1550633" ></a>
                     <!--Dev PR:1550633 Doc PR:1577805  -->
             <strong> In a vDS environment, vMotion fails after removing LAG</strong><br>
             In a vSphere Distributed Switch (vDS) environment, vMotion fails after removing Link Aggregation Groups (LAG). vMotion Wizard Compatibility shows an error message similar to the following:<br><br>
<tt>Currently connected network interface 'Network Adapter 1" uses network 'DSwitchName', which is not accessible</tt><br><br>
This issue is resolved in this release.</li>
</ul>

<ul><li><a name="1582259"  id="1582259" ></a>
                     <!--Dev PR:1582259 Doc PR:1563547  -->
             <strong>Fibre Channel over Ethernet (FCoE) link might go down when collecting ESXi log bundle</strong><br>
When collecting ESXi log bundle, the <tt>lldpnetmap</tt> command enables LLDP; however, the LLDP can be only set on Both mode and the LLDP packets are sent out by the ESXi host. The packets might cause the FCoE link to go down.<br><br>
             This issue is resolved in this release.
         </li>
</ul>

<ul><li><a name="1540863"  id="1540863" ></a>
                     <!--Dev PR:1540863 Doc PR:1602596  -->
             <strong>Hostd logs flooded with error message multiple times in quick succession</strong><br>
             Hostd logs are flooded with an error message similar to the following multiple times in quick succession filling up the logs:<br><br>
<tt>Failed to get vsi stat set: Sysinfo error on operation returned status : Not found. Please see the VMkernel log for detailed error information.</tt><br><br>
             This issue is resolved in this release.<br>
                     <br><br> </li>
</ul>

<b><a name="rsecurityissues"></a>Security Issues</b>

<ul><li><a name="1538431"  id="1538431" ></a>
                     <!--Dev PR:1538431 Doc PR:1589109  -->
             <strong>Update to the NTP package</strong><br>
             The ESXi NTP package is updated to version 4.2.8p4.<br><br></li>
</ul>

<ul><li><a name="1555277" id="1555277" ></a>
                     <!--Dev PR:1555277 Doc PR:1589332 -->
             <strong>Update to the zlib version</strong><br>
             The zlib version is updated to version 1.2.8.
         <br><br></li>
</ul>


<ul><li><a name="1580823" id="1580823" ></a>
                     <!--Dev PR:1580823 Doc PR:1589827 -->
             <strong>Update to the libPNG library</strong><br>
             The libPNG library is updated to libpng-1.6.20.<br><br></li>
</ul>

<ul><li><a name="1561776" id="1561776" ></a>
                     <!--Dev PR:1561776 Doc PR:1589360 -->
             <strong>Update to the OpenSSH version</strong><br>
             The OpenSSH version is updated to 7.1p1.<br><br></li>
</ul>

<ul><li><a name="1607985"  id="1607985" ></a>
                     <!--Dev PR:1607985 Doc PR:1594890 -->
             <strong>IO Filter VASA Provider does not complete initialization due to missing newline in certificate file</strong><br>
The IO Filter VASA Provider reads and combines the certificate and private key files to generate the pem file. If the certificate file does not end with a new line an invalid pem file is created which OpenSSL does not recognize. This in turn results in an error which prevents iofiltervp from initializing.<br>
<br>
             This issue is resolved in this release.
        <br><br></li>
</ul>

<b><a name="rserverissues"></a>Server Configuration Issues</b>

<ul><li><a name="1455020"  id="1455020" ></a>
                     <!--Dev PR:1455020 Doc PR:1583375 -->
             <strong>Unable to perform active directory operations after you join the ESXi Host to Active Directory</strong><br>
After you join the ESXi host to Active Directory, you might be unable to perform Active Directory activities like adding Active Directory Users for permission after certain addition of users from different trusted domains, log in, and so on. In the <tt>/var/log/likewise.log</tt> file, you see entries similar to:<br><br>
<tt>20150522181259:DEBUG:lsass:LsaJoinDomain():join.c:380: Error code: 40286 (symbol: LW_ERROR_LDAP_SERVER_DOWN)</tt></br></br>
This issue is resolved in this release.
         <br><br></li>
</ul>

<ul><li><a name="1455020"  id="1455020" ></a>
                     <!--Dev PR:1455020 Doc PR:1583375 -->
             <strong>Latest PCI IDs added</strong><br>
The pci.ids file is refreshed to contain the latest PCI IDs.
         <br><br></li>
</ul>

<ul><li><a name="1582877"  id="1582877" ></a>
                     <!--Dev PR:1582877 Doc PR:1603336  -->
             <strong>The ESXi host is not moved to maintenance mode in event of a resource pool allocation error</strong><br>
             The ESXi host is not moved to maintenance mode in event of a resource pool allocation error.<br><br>
            This issue is resolved in this release.
         <br><br></li>
</ul>

<b><a name="rstorageissues"></a>Storage Issues</b>

<ul><li><a name="1559781"  id="1559781" ></a>
                     <!--Dev PR:1559781 Doc PR:1578098  -->
             <strong>ESXi mClock I/O scheduler does not work as expected</strong><br>
             The ESXi mClock I/O scheduler does not limit the I/Os with a lesser load even after you change the IOPS of the VM using the vSphere Client.<br><br>
             This issue is resolved in this release.
         <br><br></li>
</ul>

<ul><li><a name="1559790"  id="1559790" ></a>
                     <!--Dev PR:1559790 Doc PR:1578336  -->
             <strong>Delayed update of path state of existing LUNs</strong><br>
            Occasionally you might observe slight delay in LUN path state update, as an internal API does not issue scan call to the existing LUNs.<br><br>
             This issue is resolved in this release.
         <br><br></li>
</ul>

<ul><li><a name="1434877"  id="1434877" ></a>
                     <!--Dev PR:1434877 Doc PR:1577966  -->
             <strong>Attempts to attach more than one Appstack takes a long time</strong><br>
             When you attempt to attach more than one Appstack or writable volume to a virtual machine, there is a delay in the reconfiguration of the VM.<br><br>
             This issue is resolved in this release.
         <br><br></li>
</ul>

<ul><li><a name="1553310"  id="1553310" ></a>
                     <!--Dev PR:1553310 Doc PR:1583400  -->
             <strong>Unable to open a more than 2 TB disk on Virtual Volumes using the VDDK HotAdd transport mode</strong><br>
             Attempts to use the Virtual Disk Development Kit (VDDK) HotAdd transport mode to HotAdd and open more than 2 TB disk on Virtual Volumes datastore might fail.<br><br>
            This issue is resolved in this release.
         <br><br></li>
</ul>
<ul><a name="iofilteripv6"></a><!-- DOCNOTES: GA 1501595 -->
<li><strong>Installing I/O Filters on IPv6 setup does not publish its capabilities to VPXD</strong><br />
After successful installation of I/O Filter through VIM API, the installed filter is not able to publish the filter capabilities to VPXD. You are unable to attach the filter profile to any disks as there are no capabilities published to the VMware vSphere Storage Policy Based Management (SPBM).<br><br>
This issue is resolved in this release.
<br><br></li></ul>
<ul><li><a name="vmiof"></a>
                     <!--Dev PR:1575550 Doc PR:1602832 -->
             <strong>ESXi 6.0 Update 2 supports VMIOF versions 1.0 and 1.1</strong><br><br>
ESXi 6.0 Update 2 supports VMIOF versions 1.0 and 1.1. The <tt>vmiof_1_1_0_0</tt> vib tag is added to the list of supported VIB tags in the ESXi base image. This allows you to create filters that are compatible only with ESXi 6.0 Update 2 and later, as the filters created on the ESXi 6.0 Update 2 devkit will only install on ESXi 6.0 Update 2 or later hosts.<br><br>
Filters created on ESXi 6.0 Update 2 will also work with ESXi 6.0 Update hosts without any issues.
<br><br></li>
</ul>

<ul><li><a name="1587500"  id="1587500" ></a>
                     <!--Dev PR:1587500 Doc PR:1603410 -->
             <strong>After upgrade of hosts in a Virtual SAN cluster to ESXi 6.0 Update 1b, some hosts in the cluster might report false warning</strong><br>
             After you upgrade Virtual SAN environment to ESXi 6.0 Update 1b, the vCenter Server reports a false warning similar to the following in the <strong>Summary</strong> tab in the vSphere Web Client and the ESXi host shows a notification triangle:<br><br>
<tt>Host cannot communicate with all other nodes in Virtual SAN enabled cluster</tt><br><br>
            This issue is resolved in this release.
         <br><br></li>
</ul>



<!--<b><a name="rhardwareissues"></a>Supported Hardware Issues</b>-->

<b><a name="rupgradeinstallissues"></a>Upgrade and Installation Issues</b>
<ul><li><a name="1570452" id="1570452" ></a>
                     <!--Dev PR:1570452 Doc PR:1585823  -->
             <strong>ESXi 6.x scripted installation incorrectly warns that USB or SD media does not support VMFS, even after --novmfsondisk parameter is included in kickstart file</strong><br>
             If you use scripted installation to install ESXi 6.0 on a disk that is identified as USB or SD media, the installer might display the warning message:<br><br>
			 <tt>The disk (&#60;disk-id&#62;) specified in install does not support VMFS.</tt><br><br>
             This message is displayed even if you have included the <tt>--novmfsondisk</tt> parameter for the install command in the kickstart file.<br><br>
			 This issue is resolved in this release.
         <br><br></li>
</ul>

<ul><li><a name="1477248" "1477257"  id="11477248" "1477257" ></a>
                     <!--Dev PR:1477248, 1477257 Doc PR:1578008  -->
             <strong>Installed Live VIB might not function properly and be lost after a reboot</strong><br>
             During Live VIB installation, jumpstart plug-ins, rc scripts and service scripts of the Live VIB are executed. These plugins and scripts might throw error and/or exception. The errors and/or exceptions might stop the installation process and cause unexpected behavior such as the Live VIB is reported as installed on the system, but does not function properly. Also, the Live VIB is lost after a reboot.<br>
             <br>
             This issue is resolved in this release.
         <br><br></li>
</ul>

<ul><li><a name="1552199"  id="1552199" ></a>
                     <!--Dev PR:1552199 Doc PR:1578345  -->
             <strong>Live VIB installation might fail</strong><br>
             During the Live VIB installation, esximage creates stage data for Live VIB. If you run the esxcli software VIB <tt>get</tt> and <tt>list</tt> command at the same time, the stage data might get deleted and cause the Live VIB installation transaction to fail.<br><br>
             This issue is resolved in this release.
         <br><br></li>
</ul>

<ul><li><a name="1600284"  id="1600284" ></a>
                     <!--Dev PR:1600284 Doc PR:1607457 -->
             <strong>Attempts to perform vMotion might fail after you upgrade from ESXi 5.0 or 5.1 to 6.0 Update 1 and later releases</strong><br><br>
Attempts to perform vMotion might fail after you upgrade from ESXi 5.0 or 5.1 to 6.0 Update 1 or later releases. An error message similar to the following is written to the <tt>vmware.log</tt> file.<br><br>
<tt>failed to add memory page 0x201 to VM: Bad parameter</tt><br><br>
For further details, see <a href="http://kb.vmware.com/kb/2143943">Knowledge Base article 2143943</a>.<br><br>
This issue is resolved in this release.<br><br></li>
</ul>

<b><a name="rclientissues"></a>vCenter Server, vSphere Web Client, and vSphere Client Issues</b>

<ul><li><a name="1540114"  id="1540114" ></a>
                     <!--Dev PR:1540114 Doc PR:1578342  -->
             <strong>Unable to open VM console connection to the powered on VMs from vSphere Web Access</strong><br>
When you add an ESXi host with powered on VMs to the vCenter Server or when you switch the host from one vCenter Server to another, the vSphere Web Access is unable to open console connection to the VMs. Error messages similar to the following are observed with this issue:<br><br>
Web console error message:<br><br>
<tt>The console has been disconnected. Close this window and re-launch the console to reconnect.</tt><br><br>
Standalone VMware Remote Console error message:<br><br>
<tt>Failed to initialize SSL session to remote host.</tt><br><br>
VI client console error message:<br><br>
<tt>unable to connect to the mks: internal error</tt><br><br>
Error message in the <tt>vmware.log</tt> file:<br><br>
<tt>SOCKET 6 (150) Error during authd-VNC negotiation: (1) Asyncsocket error.</tt><br><br>
             This issue is resolved in this release.<br><br>
         </li>
</ul>

<!--<b><a name="rvcenterssocmissues"></a>vCenter Single Sign-On and Certificate Management Issues</b>-->

<b><a name="#rvmissues"></a>Virtual Machine Management Issues</b>

<ul><li><a name="1560820"  id="1434877" ></a>
                     <!--Dev PR:1434877 Doc PR:1578337  -->
             <strong>Hostd randomly stops responding on hosts with 3D acceleration</strong><br>
Hostd might randomly stop responding on ESXi hosts with 3D acceleration. Messages similar to the following are logged in the hostd log:<br><br>
<tt>
Crash Report build=2809209<br>
Signal 11 received, si_code 1, si_errno 0<br>
Bad access at 34<br>
eip 0xnnnnnnn esp 0xnnnnnnnn ebp 0xnnnnnnnn<br>
eax 0xnnnnnnnn ebx 0xnnnnnnn ecx 0x47 edx 0x0 esi 0x30 edi 0xnnnnnnnn<br>
</tt>            
 This issue is resolved in this release.<br><br>
         </li>
</ul>

<ul><li><a name="1128579"  id="1128579" ></a>
                     <!--Dev PR:1128579 Doc PR:1584058  -->
             <strong>Virtual machines affected by Permanent Device Loss do not failover</strong><br>
             Virtual machines affected by Permanent Device Loss (PDL) do not power on in a failover host. The VM is left in an invalid state with a warning message stating that Hardware Version 1 is not recognized whenever there is a PDL of any of the storage hosting VM.<br><br>
             This issue is resolved in this release.<br><br>
         </li>
</ul>

<ul><li><a name="1580173"  id="1580173" ></a>
                     <!--Dev PR:1580173 Doc PR:1584069  -->
             <strong>Performance counter cpu.system.summation for a VM is always displayed as 0</strong><br>
             Virtual machine performance metrics are not displayed correctly as the performance counter <tt>cpu.system.summation</tt> for a VM is always displayed as 0.<br><br>
             This issue is resolved in this release.<br><br>
         </li>
</ul>


<!--<b><a name="rvsanissues"></a>Virtual SAN Issues</b>-->

<b><a name="rvmotionstorage"></a>vMotion and Storage vMotion Issues</b>

<ul><li><a name="1555240"  id="1555240" ></a>
                     <!--Dev PR:1555240 Doc PR:1583405  -->
             <strong>Hostd might stop responding when the ESXi host with 3D hardware is put on maintenance mode and vMotion is triggered</strong><br>
             On an ESXi host that has 3D hardware, when hostd detects power state change of a vm, a function is called to check the vm state. In the case of vMotion, the source VM is powered off before being unregistered on the source host, the ManagedObjectNotFound exception is displayed and the hostd might stop responding.<br><br>
            This issue is resolved in this release.<br><br>
         </li>
</ul>


<ul><li><a name="1526666"  id="1526666" ></a>
                     <!--Dev PR:1526666 Doc PR:1592397 -->
             <strong>Unable to perform vMotion with VMs that has two 2TB virtual disks</strong><br>
Attempts to perform vMotion with ESXi 6.0 virtual machines that have two 2 TB virtual disks created on ESXi 5.0 fail with an error messages similar to the following logged in the <tt>vpxd.log</tt> file:<br><br>
<tt>2015-09-28T10:00:28.721+09:00 info vpxd[xxxxx] [Originator@6876 sub=vpxLro opID=xxxxxxxx-xxxxxxxx-xx] [VpxLRO] -- BEGIN task-919 -- vm-281 -- vim.VirtualMachine.relocate -- xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx(xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx)</tt><br>
<tt>2015-09-28T10:00:28.727+09:00 info vpxd[xxxxx] [Originator@6876 sub=vpxLro opID=xxxxxxxx-xxxxxxxx-xx-xx] [VpxLRO] -- BEGIN task-internal-343729 --  -- VmprovWorkflow -- </tt><br>
<tt>....</tt><br><br>
             This issue is resolved in this release.<br><br>
         </li>
</ul>

<ul><li><a name="1500099"  id="1500099" ></a>
                     <!--Dev PR:1500099 Doc PR:1578011  -->
             <strong>Virtual machine stops responding or goes into power off state during disk-only storage vMotion</strong><br>
             When you perform a disk-only storage vMotion, the VM might stop responding or go into a power off state as multiple threads attempt to access the same data.<br><br>
             This issue is resolved in this release.<br><br>
                      </li>
</ul>

<!--<b><a name="rvmwarehaissues"></a>VMware HA and Fault Tolerance Configuration Issues</b>-->

<b><a name="rvmtoolsissues"></a>VMware Tools Issues</b>
<ul><li><a name="1588701"  id="1588701" ></a>
                     <!--Dev PR:1588701 Doc PR:1588701  -->
             <strong>VMware Tools version 10.0.6 included</strong><br>
             This release includes the VMware Tools version 10.0.6. Refer to the <a href="https://pubs.vmware.com/Release_Notes/en/vmwaretools/1006/vmware-tools-1006-release-notes.html">VMware Tools 10.0.6 Release Notes</a> for further details.<br><br></li>
</ul>

<!--<b><a name="rvicli_issues"></a>vSphere Command-Line Interface Issues</b>-->


<h2><a name="knownissues" id="knownissues"></a>Known Issues</h2>
<p>The known issues existing in ESXi 6.0 are grouped as follows:</p> 
<ul>
<li><a href="#installissues">Installation Issues</a></li>
<li><a href="#upgradeissues">Upgrade Issues</a></li>
<!--<li><a href="#licensingissues">Licensing Issues</a></li>-->
<li><a href="#vcenterssocmissues">vCenter Single Sign-On and Certificate Management Issues</a></li>
<li><a href="#networkingissues">Networking Issues</a></li>
<li><a href="#storageissues">Storage Issues</a></li>
<!--<li><a href="#vsanissues">Virtual SAN Issues</a></li>
<li><a href="#backupissues">Backup Issues</a></li>-->
<li><a href="#serverissues">Server Configuration Issues</a></li>
<!--<li><a href="#clientissues">vCenter Server, vSphere Web Client, and vSphere Client Issues</a></li>
<li><a href="#vmissues">Virtual Machine Management Issues</a></li>
<!-- <li><a href="#migrationissues">Migration Issues</a></li> -->
<li><a href="#vmwarehaissues">VMware HA and Fault Tolerance Issues</a></li> 
<li><a href="#gosissues">Guest Operating System Issues</a></li>
<li><a href="#hardwareissues">Supported Hardware Issues</a></li>
<!-- <li><a href="#i18nissues">Internationalization Issues</a></li> -->
<!-- <li><a href="#vicli_issues">vSphere Command-Line Interface Issues</a></li>--> 
<!--<li><a href="#adib_issues">Auto Deploy and Image Builder Issues</a></li>-->
<li><a href="#cimapiissues">CIM and API Issues</a></li>
<!--<li><a href="#miscissues">Miscellaneous Issues</a></li>
<!--<li><a href="#vmtoolsissues">VMware Tools Issues</a></li>-->
</ul>
<p>New known issues documented in this release are highlighted as <font color=#CC0000><strong>New Issue</strong></font>.</p>

<b><a name="installissues"></a>Installation Issues</b>

<ul>

<!--DOCNOTES 1572704-->
<li><p><strong>DNS suffix might persist even after you change the default configuration in DCUI</strong><br />
An ESXi host might automatically get configured with the default DNS + DNS suffix on first boot, if deployed on a network served by a DHCP server. When you attempt to change the DNS suffix, the DCUI does not remove the existing DNS suffix but just adds the new suffix provided as well.</p>
<p>Workaround: When configuring DNS hostname of the witness OVF, set the FULL FQDN name in the DNS Hostname field to append the correct DNS suffix. You can then remove unwanted DNS suffixes in the Custom DNS Suffix field.</p>
</li>

</ul>
<ul>

<!--DOCNOTES 1494726-->
<li><p><strong>The VMware Tools service user processes might not run on Linux OS after installing the latest VMware Tools package</strong><br />
On Linux OS, you might encounter VMware Tools upgrade/installation issues or the VMware Tools service (vmtoolsd) user processes might not run after installing the latest VMware Tools package. The issue occurs if the your glibc version is older than version 2.5, like SLES10sp4.</p>
<p>Workaround: Upgrade the Linux glibc to version 2.5 or above.</p>
</li>

</ul>


<b><a name="upgradeissues"></a>Upgrade Issues</b>
<p>Review also the Installation Issues section of the release notes. Many installation issues can also impact your upgrade process.</p>

<ul>
<!--DOCNOTES 1603362-->
<li><p><font color=#CC0000><strong>New Issue</strong></font><strong>  Attempts to upgrade from ESXi 6.x to 6.0 Update 2 with the esxcli software vib update command fail</strong><br />
Attempts to upgrade from ESXi 6.x to 6.0 Update 2 with the <tt>esxcli software vib update</tt> fails with error messages similar to the following:<br><br>
<tt>"[DependencyError]<br>
  VIB VMware_bootbank_esx-base_6.0.0-2.34.xxxxxxx requires vsan &#60;&#60; 6.0.0-2.35, but the requirement cannot be satisfied within the ImageProfile.<br>
  VIB VMware_bootbank_esx-base_6.0.0-2.34.xxxxxxx requires vsan &#62;= 6.0.0-2.34, but the requirement cannot be satisfied within the ImageProfile."</tt><br><br>
  The issue occurs due to introduction of a new Virtual SAN VIB which is interdependent with the esx-base VIB and the <tt>esxcli software vib update</tt> command only updates the VIBs already installed on the system.
</p>
<p>Workaround: To resolve this issue, run the <tt>esxcli software profile update</tt> as shown in the following example:<br><br>
<tt>esxcli software profile update -d /vmfs/volumes/datastore1/update-from-esxi6.0-6.0_update02.zip -p ESXi-6.0.0-20160302001-standard</tt>
</p></li>

<!--DOCNOTES 1609163-->
<li>
  <p><font color=#CC0000><strong>New Issue</strong></font><strong>  Attempts to upgrade ESXi might fail when the ESXi partition table contains a coredump partition</strong><br />
When ESXi is deployed on an SD card using the <tt>'dd'</tt> command (deploy ESXi using the <tt>.dd</tt> image created using <tt>esxiso2dd</tt> command provided by VMware), ESXi creates a coredump partition as a second partition during the first boot. As a result, the ESXi upgrade fails.<br><br>
Workaround: To resolve this issue, you need to remove the second coredump partition manually. See <a href="http://kb.vmware.com/kb/2144074">Knowlege Base article 2144074</a>.
</p>
</li>


<!--DOCNOTES 1516263-->
<li><p><font color=#CC0000><strong>Updated</strong></font><strong> SSLv3 remains enabled on Auto Deploy after upgrade from earlier release of vSphere 6.0 to vSphere 6.0 Update 1 and above</strong><br />
When you upgrade from an earlier release of vSphere 6.0 to vSphere 6.0 Update 1 and above, the SSLv3 protocol remains enabled on Auto Deploy.</p>
<p>Workaround: Perform to the following steps to disable SSLv3 using PowerCLI commands:
<ol>
<li>Run the following command to Connect to vCenter Server:</br></br>
<tt>PowerCLI C:\Program Files (x86)\VMware\Infrastructure\vSphere PowerCLI&#62; Connect-VIServer -Server &#60;FQDN_hostname or IP Address of vCenter Server&#62;</tt></br></br></li>
<li>Run the following command to check the current sslv3 status:</br></br>
<tt>PowerCLI C:\Program Files (x86)\VMware\Infrastructure\vSphere PowerCLI&#62; Get-DeployOption</tt></br></br></li>
<li>Run the following command to disable sslv3:</br></br>
<tt>PowerCLI C:\Program Files (x86)\VMware\Infrastructure\vSphere PowerCLI&#62; Set-DeployOption disable-sslv3 1</tt></br></br></li>
<li>Restart the Auto Deploy service to update the change.</li></ol>
</p>
</li>


<!-- DOCNOTES: 1312179  -->
<li><strong>Fibre Channel host bus adapter device number might change after ESXi upgrade from 5.5.x to 6.0</strong><br/>
<p>During ESXi upgrade from 5.5.x to 6.0, the Fibre Channel host bus adapter device number changes occasionally. The device number might change to another number if you use the <code>esxcli storage core adapter list</code> command.</p>
<p>For example, the device numbers for a Fibre Channel host bus adapter might look similar to the following before ESXi upgrade:</p>
<p><tt>HBA Name<br>                       
&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;<br>  
vmhba3<br>
vmhba4<br>
vmhba5<br>
vmhba66</tt><br>
<p>The device numbers from the Fibre Channel host bus adapter might look similar to the following after an ESXi upgrade 6.0:</p>
<p><tt>HBA Name<br>
&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;&ndash;<br>
vmhba64<br>
vmhba65<br>
vmhba5<br>
vmhba6</tt><br>
<p>The example illustrates the random change that might occur if you use the <code>esxcli storage core adapter list</code> command: the device alias numbers vmhba2 and vmhba3 change to vmhba64 and vmhba65, while device numbers vmhba5 and vmhba6 are not changed. However, if you used the <code>esxcli hardware pci list</code> command, the device numbers do not change after upgrade.</p>
<p>This problem is external to VMware and may not affect you. ESXi displays device alias names but it does not use them for any operations. You can use the host profile to reset the device alias name. Consult VMware product documentation and knowledge base articles.</p>
<p>Workaround: None.</p>
</li>


<!-- DOCNOTES: 1199376 -->
<li><p><strong>Active Directory settings are not retained post-upgrade</strong><br/>
The Active Directory settings configured in the ESXi host before upgrade are not retained when the host is upgraded to ESXi 6.0.</p>
<p>Workaround: Add the host to the Active Directory Domain after upgrade if the pre-upgrade ESXi version is 5.1 or later. Do not add the host to the Active Directory Domain after upgrade if the pre-upgrade ESXi version is ESXi 5.0.x.</p>
</li>


<!-- DOCNOTES: 1369897 -->
<li><p><strong>After ESXi upgrade to 6.0 hosts that were previously added to the domain are no longer joined to the domain</strong><br/>
When upgrading to from vSphere 5.5 to  vSphere 6.0 for the first time, the Active Directory configuration is not retained.</p>
<p>Workaround: After upgrade, rejoin the hosts to the vCenter Server domain:</p>
<ol><li><p>Add the hosts to vCenter Server.</p></li>
<li><p>Join the hosts to domain (for example, example.com)</p></li>
<li><p>Upgrade all the hosts to ESXi 6.0.</p></li>
<li><p>Manually join one recently upgraded host to domain.</p></li>
<li><p>Extract the host profile and disabled all other profiles except Authentication.</p></li>
<li><p>Apply the manually joined host profile to the other recently upgraded hosts.</p></li>
</ol>
</li>


<!-- DOCNOTES: 1361117 -->
<li><p><strong>Previously running VMware ESXi Dump Collector service resets to default Disabled setting after upgrade of vCenter Server for Windows</strong><br/>
The upgrade process installs VMware Vsphere ESXi Dump Collector 6.0 as part of a group of optional services for vCenter Server. You must manually enable the VMware vSphere ESXi Dump Collector service to use it as part of vCenter Server 6.0 for Windows.</p>
<p>Workaround: Read the VMware documentation or search the VMware Knowledge Base for information on how to enable and run optional services in vCenter Server 6.0 for Windows.</p>
<p> Enable the VMware vSphere ESXi Dump Collector service in the operating system:</p>
<ol>
<li><p>In the Control Panel menu, select <strong>Administrative Tools</strong> and double-click on <strong>Services</strong>.</p></li>
<li><p>Right click <strong>VMware vSphere ESXi Dump Collector</strong> and <strong>Edit Startup Type</strong>.</p></li>
<li><p>Set the <strong>Start-up Type</strong> to <strong>Automatic</strong>.</p></li>
<li><p>Right Click <strong>VMware vSphere ESXi Dump Collector</strong> and <strong>Start</strong>.</p></li>
</ol>
<p>The <strong>Service Start-up Type</strong> is set to automatic and the service is in a running state.</p>
</li>
</ul>

<!--<b><a name="licensingissues"></a>Licensing Issues</b> -->

<b><a name="vcenterssocmissues"></a>vCenter Single Sign-On and Certificate Management Issues</b>

<ul>

<!-- DOCNOTES:1309954 Still an issue in GA  & Approved in RC-->
<li><p><strong>Cannot connect to VM console after SSL certificate upgrade of ESXi host</strong><br />
A certificate validation error might result if you upgrade the SSL certificate that is used by an ESXi host, and you then attempt to connect to the VM console of any VM running when the certificate was replaced. This is because the old certificate is cached, and any new console connection is rejected due to the mismatch. <br />
The console connection might still succeed, for example, if the old certificate can be validated through other means, but is not guaranteed to succeed. Existing virtual machine console connections are not affected, but you might see the problem if the console was running during the certificate replacement, was stopped, and was restarted.  </p>
<p>Workaround: Place the host in maintenance mode or suspend or power off all VMs. Only running VMs are affected.  
As a best practice, perform all SSL certificate upgrades after placing the host in maintenance mode.  <br />
</p>
</li> 
</ul>

<p><b><a name="networkingissues"></a>Networking Issues</b></p> 
<ul>
<li>
  <p><strong>Certain vSphere functionality does not support IPv6</strong><br />
  You can enable IPv6 for all nodes and components except for the following features:</p>
  <ul>
  <li>
    <p>IPv6 addresses for ESXi hosts and vCenter Server that are not mapped to fully qualified domain names (FQDNs) on the DNS server. <br />
      Workaround:
      Use FQDNs or  make sure the IPv6 addresses are mapped to FQDNs on the DNS servers for reverse name lookup.</p>
  </li>
  <li>
    <p>Virtual volumes</p>
  </li>
  <li>
    <p>PXE booting as a part of Auto Deploy and Host Profiles<br/>
	Workaround: PXE boot an ESXi host over IPv4 and configure the host for IPv6 by using Host Profiles.</p>
  </li>
  <li>
    <p> Connection of ESXi hosts and the vCenter Server Appliance to Active Directory<br /> 
      Workaround: Use Active Directory over LDAP as an identity source in vCenter Single Sign-On.</p>
  </li>
  <li>
    <p> NFS 4.1 storage with Kerberos<br /> 
      Workaround: Use NFS 4.1 with AUTH_SYS.</p>
  </li>
  <li>
    <p> Authentication Proxy</p>
  </li>
  <li>
    <p> Connection of the vSphere Management Assistant and vSphere Command-Line Interface to Active Directory.<br /> 
      Workaround: Connect to Active Directory over LDAP.</p>
  </li>
  <li>
    <p>Use of the vSphere Client to enable IPv6 on vSphere features<br />
      Workaround: Use the vSphere Web Client to enable IPv6 for vSphere features.</p>
  </li>
  </ul>

  </li>

<!-- DOCNOTES: 1365420 -->
<li><p><strong>Recursive panic might occur when using ESXi Dump Collector</strong><br />
Recursive kernel panic might occur when the host is in panic state while it displays the purple diagnostic screen and write the core dump over the network to the ESXi Dump Collector. A VMkernel zdump file might not be available for troubleshooting on the ESXi Dump Collector in vCenter Server.</p>
<p>In the case of a recursive kernel panic, the purple diagnostic screen on the host displays the following message:<br/>
<code>2014-09-06T01:59:13.972Z cpu6:38776)Starting network coredump from <em>host_ip_address</em> to <em>esxi_dump_collector_ip_address</em>.<br />
  [7m2014-09-06T01:59:13.980Z cpu6:38776)WARNING: Net: 1677: Check what type of stack we are running on [0m<br />
  Recursive panic on same CPU (cpu 6, world 38776, depth 1): ip=0x418000876a27 randomOff=0x800000:<br />
  #GP Exception 13 in world 38776:vsish @ 0x418000f0eeec<br />
  Secondary panic trap frame registers:<br />
  RAX:0x0002000001230121 RCX:0x000043917bc1af80 RDX:0x00004180009d5fb8 RBX:0x000043917bc1aef0<br />
  RSP:0x000043917bc1aee8 RBP:0x000043917bc1af70 RSI:0x0002000001230119 RDI:0x0002000001230121<br />
  R8: 0x0000000000000038 R9: 0x0000000000000040 R10:0x0000000000010000 R11:0x0000000000000000<br />
  R12:0x00004304f36b0260 R13:0x00004304f36add28 R14:0x000043917bc1af20 R15:0x000043917bc1afd0<br />
  CS: 0x4010 SS: 0x0000 FS: 0x4018 GS: 0x4018 IP: 0x0000418000f0eeec RFG:0x0000000000010006<br />
  2014-09-06T01:59:14.047Z cpu6:38776)Backtrace for current CPU #6, worldID=38776, rbp=0x43917bc1af70<br />
  2014-09-06T01:59:14.056Z cpu6:38776)0x43917bc1aee8:[0x418000f0eeec]do_free_skb@com.vmware.driverAPI#9.2+0x4 stack: 0x0, 0x43a18b4a5880,<br />
  2014-09-06T01:59:14.068Z cpu6:38776)Recursive panic on same CPU (cpu 6, world 38776): ip=0x418000876a27 randomOff=0x800000:<br />
  #GP Exception 13 in world 38776:vsish @ 0x418000f0eeec<br />
  Halt$Si0n5g# PbC8PU 7.</code></p>
  <p>Recursive kernel panic might occur when the VMkernel panics while heavy traffic is passing through the physical network adapter that is also configured to send the core dumps to the collector on vCenter Server.</p>
<p>Workaround: Perform either of the following workarounds:</p>
<ul>
  <li><p>Dedicate a physical network adapter to core dump transmission only to reduce the impact from system and virtual machine traffic.</p></li>
  <li><p>Disable the ESXi Dump Collector on the host by running the following ESXCLI console command:<br />
    <code>esxcli system coredump network set --enable false</code></p></li>
</ul>
</li>
</ul>


<b><a name="storageissues"></a>Storage Issues</b><br>

<p><b><i>NFS Version 4.1 Issues</i></b></p>



<ul>

<!-- DOCNOTES: GA 1261991 -->
<li><p><strong>Virtual machines on an NFS 4.1 datastore fail after the NFS 4.1 share recovers from an all paths down (APD) state
 </strong><br />
When the NFS 4.1 storage enters an APD state and then exits it after a grace period, powered on virtual machines that run on the NFS 4.1 datastore fail. The grace period depends on the array vendor.<br />


After the NFS 4.1 share recovers from APD, you see the following message on the virtual machine summary page in the vSphere Web Client:<br />

<code>The lock protecting VM.vmdk has been lost, possibly due to underlying storage issues. If this virtual machine is configured to be highly available, ensure that the virtual machine is running on some other host before clicking OK. </code><br />


After you click OK, crash files are generated and the virtual machine powers off.



</p>
<p>Workaround: None.
</p>
</li>


<!-- DOCNOTES: GA 1369513 GG-->
<li><p><strong>NFS 4.1 client loses synchronization with server when trying to create new sessions </strong><br />
After a period of interrupted connectivity with the server, the NFS 4.1 client might lose synchronization with the server when trying to create new sessions. When this occurs, the <code>vmkernel.log</code> file contains a throttled series of warning messages noting that an NFS41 CREATE_SESSION request failed with NFS4ERR_SEQ_MISORDERED.
</p>
<p>Workaround: 
Perform the following sequence of steps.

<OL>
<LI><p>Attempt to unmount the affected file systems.  If no files are open when you unmount, this operation succeeds and the NFS client module cleans up its internal state.  You can then remount the file systems that were unmounted and resume normal operation.</p></LI>

<LI><p>Take down the NICs connecting to the mounts' IP addresses and leave them down long enough for several server lease times to expire. Five minutes should be sufficient.  You can then bring the NICs back up. Normal operation should resume.</p></LI>

<LI><p>If the preceding steps fail, reboot the ESXi host.</p></LI></OL>

 </p>
</li>


<!-- DOCNOTES: GA 1369544 GG-->
<li><p><strong>NFS 4.1 client loses synchronization with an NFS server and connection cannot be recovered even when session is reset </strong><br />
After a period of interrupted connectivity with the server, the NFS 4.1 client might lose synchronization with the server and the synchronized connection with the server cannot be recovered even if the session is reset. This problem is caused by an EMC VNX server issue. 

When this occurs, the <code>vmkernel.log</code> file contains a throttled series of warning messages noting that NFS41: NFS41ProcessSessionUp:2111: resetting session with mismatched clientID; probable server bug
</p>

  <p>Workaround:  To end the session, unmount all datastores and then remount them.  </p>
</li>



<!-- DOCNOTES: GA 1382435 GG-->
<li><p><strong>ONTAP Kerberos volumes become inaccessible or experience VM I/O failures  </strong><br />
A NetApp server does not respond when it receives RPCSEC_GSS requests that arrive out of sequence. As a result, the corresponding I/O operation stalls unless it is terminated and the guest OS can stall or encounter I/O errors. Additionally, according to RFC 2203, the client can only have a number of outstanding requests equal to seq_window (32 in case of ONTAP) according to RPCSEC_GSS context and it must wait until the lowest of these outstanding requests is completed by the server. Therefore, the server never replies to the out-of-sequence RPCSEC_GSS request, and the client stops sending requests to the server after it reaches the maximum seq_window number of outstanding requests. This causes the volume to become inaccessible.</p>
<p>Workaround: None. Check the latest Hardware Compatibility List (HCL) to find a supported ONTAP server that has resolved this problem.</p>
</li>


<!-- DOCNOTES: GA 1359261 SV -->
<li><p><strong>You cannot create a larger than 1 TB virtual disk on NFS 4.1 datastore from EMC VNX </strong><br />
NFS version 4.1 storage from EMC VNX with firmware version 7.x supports only 32-bit file formats. This prevents you from creating virtual machine files that are larger than 1 TB on the NFS 4.1 datastore. 

</p>
<p>Workaround: Update the EMC VNX array to version 8.x.
</p>
</li>



<!-- DOCNOTES: GA 1362788 SV -->
<li><p><strong>NFS 4.1 datastores backed by EMC VNX storage become inaccessible during firmware upgrades</strong><br />
When you upgrade EMC VNX storage to a new firmware, NFS 4.1 datastores mounted on the ESXi host become inaccessible. This occurs because the VNX server changes its major device number after the firmware upgrade. The NFS 4.1 client on the host does not expect the major number to change after it has established connectivity with the server, and causes the datastores to be permanently inaccessible.</p>
<p>Workaround: Unmount all NFS 4.1 datastores exported by the VNX server before upgrading the firmware.</p>
</li>


<!-- DOCNOTES: GA 1391372 SV -->
<li><p><strong>When ESXi hosts use different security mechanisms to mount the same NFS 4.1 datastore, virtual machine failures might occur

</strong><br />
If different ESXi hosts mount the same NFS 4.1 datastore using different security mechanisms, AUTH_SYS and Kerberos, virtual machines placed on this datastore might experience problems and failure. For example, your attempts to migrate the virtual machines from host1 to host2 might fail with permission denied errors. You might also observe these errors when you attempt to access a host1 virtual machine from host2. 

</p>
<p>Workaround: Make sure that all hosts that mount an NFS 4.1 volume use the same security type.
</p>
</li>


<!-- DOCNOTES: GA 1388434 SV -->
<li><p><strong>Attempts to copy read-only files to NFS 4.1 datastore with Kerberos fail</strong><br />
The failure might occur when you attempt to copy data from a source file to a target file. The target file remains empty.

</p>
<p>Workaround: None.
</p>
</li>

<!-- DOCNOTES: GA 1391379 SV -->
<li><p><strong>When you create a datastore cluster, uniformity of NFS 4.1 security types is not guaranteed</strong><br />
While creating a datastore cluster, vSphere does not verify and enforce the uniformity of NFS 4.1 security types. As a result, datastores that use different security types, AUTH_SYS and Kerberos, might be a part of the same cluster. If you migrate a virtual machine from a datastore with Kerberos to a datastore with AUTH_SYS, the security level for the virtual machine becomes lower.<br />
This issue applies to such functionalities as vMotion, Storage vMotion, DRS, and Storage DRS. 


</p>
<p>Workaround: If Kerberos security is required for your virtual machines, make sure that all NFS 4.1 volumes that compose the same cluster use only the Kerberos security type. Do not include NFS 3 datastores, because NFS 3 supports only AUTH_SYS.
</p>
</li>


</ul>
<p><b><i>Virtual Volumes Issues</i></b></p>
<ul>

<!-- DOCNOTES: GA 1355602 SV -->
<li><p><strong>Failure to create virtual datastores due to incorrect certificate used by Virtual Volumes VASA provider</strong><br />
Occasionally, a self-signed certificate used by the Virtual Volumes VASA provider might incorrectly define the <code>KeyUsage</code> extension as critical without setting the <code>keyCertSign</code> bit.  In this case, the provider registration succeeds. However,  you are not able to create a virtual datastore from storage containers reported by the VASA provider.</p>
<p>Workaround: Self-signed certificate used by the VASA provider at the time of provider registration should not define <code>KeyUsage</code> extension as critical without setting the <code>keyCertSign</code> bit.</p>
</li>
</ul>


<p><b><i>General Storage Issues</i></b></p>


<ul>
<!-- DOCNOTES: GA 1585801 -->
<li><p><font color=#CC0000><strong>New Issue</strong></font><strong> ESXi 6.0 Update 2 hosts connected to certain storage arrays with a particular version of  the firmware might see I/O timeouts and subsequent aborts</strong><br />
When ESXi 6.0 Update 2 hosts connected to certain storage arrays with a particular version of  the firmware send requests for SMART data to the storage array, and if the array responds with a PDL error, the PDL response behavior in 6.0 update 2 might result in a condition where these failed commands are continuously retried thereby blocking other commands. This error results in widespread I/O timeouts and subsequent aborts.<br><br>
Also, the ESXi hosts might take a long time to reconnect to the vCenter Server after reboot or the hosts might go into a <tt>Not Responding</tt> state in the vCenter Server. Storage-related tasks such as HBA rescan might take a very long time to complete.</p>
<p>Workaround: To resolve this issue, see <a href="http://kb.vmware.com/kb/2133286">Knowledge Base article 2133286</a>.</p>
</li>

<!-- DOCNOTES: GA 1431811 -->
<li><p><strong>vSphere Web Client incorrectly displays Storage Policy as attached when new VM is created from an existing disk</strong><br />
When you use the vSphere Web Client to create a new VM from an existing disk and specify a storage policy when setting up the disk.  The filter appears to be attached when you select the new VM --> click on <strong>VM policies</strong> --> <strong>Edit VM storage policies</strong>, however the filter is not actually attached. You can check the <tt>.vmdk</tt> file or the <tt>vmkfstools --iofilterslist &#60;vmdk-file&#62;</tt>' to verify if the filter is attached or not.</p>
<p>Workaround: After you create the new VM, but before you power it on, add the filter to the vmdk by clicking on <strong>VM policies</strong> --> <strong>Edit VM storage policies</strong>.</p>
</li>


<!-- DOCNOTES: GA 1505796 -->
<li><p><strong>NFS Lookup operation returns NFS STALE errors</strong><br />
When you deploy large number of VMs in the NFS datastore, the VM deployment fails with an error message similar to the following due to a race condition:<br><br>
<tt>Stale NFS file handle</tt></p>
<p>Workaround: Restart the Lookup operation. See <a href="http://kb.vmware.com/kb/2130593">Knowledge Based article 2130593</a> for details.</p>
</li>


<!-- DOCNOTES: GA 1263968 -->
<li><p><strong>Attempts to create a VMFS datastore on Dell EqualLogic LUNs fail when QLogic iSCSI adapters are used</strong><br />
You cannot create a VMFS datastore on a Dell EqualLogic storage device that is discovered through QLogic iSCSI adapters.<br />
When your attempts fail, the following error message appears on vCenter Server:  <code>Unable to create Filesystem, please see VMkernel log for more details: Connection timed out</code>.
The VMkernel log contains continuous <code>iscsi session blocked</code> and <code>iscsi session unblocked</code> messages.
On the Dell EqualLogic storage array, monitoring logs show a <code>protocol error in packet received from the initiator</code> message for the QLogic initiator IQN names.</p>
  <p>This issue is observed when you use the following components:</p>
<ul>
<li><p>Dell EqualLogic array firmware : V6.0.7</p></li>
<li><p>QLogic iSCSI adapter firmware versions : 3.00.01.75</p></li>
<li><p>Driver version : 5.01.03.2-7vmw-debug</p></li>
</ul>
<p>Workaround: Enable the <b>iSCSI ImmediateData</b> adapter parameter on QLogic iSCSI adapter. By default, the parameter is turned off.
You cannot change this parameter from the vSphere Web Client or by using esxcli commands. To change this parameter, use the vendor provided software, such as QConvergeConsole CLI.</p>
</li>



<!-- DOCNOTES: GA 1178816 -->
<li><p><strong>ESXi host with Emulex OneConnect HBA fails to boot</strong><br />
When an ESXi host has the Emulex OneConnect HBA installed, the host might fail to boot. This failure occurs due to a problem with the Emulex firmware.</p>
<p>Workaround: To correct this problem, contact Emulex to get the latest firmware for your HBA.</p>
<p>If you continue to use the old firmware, follow these steps to avoid the boot failure:</p>
<ol>
<li><p>When ESXi is loading, press Shift+O before booting the ESXi kernel.</p></li>
<li><p>Leave the existing boot option as is, and add a space followed by <code>dmaMapperPolicy=false</code>.</p></li>
</ol>
</li>



<!-- DOCNOTES: GA 1062515 -->
<li><p><strong>Flash Read Cache does not accelerate I/Os during APD</strong><br />
When the flash disk configured as a virtual flash resource for Flash Read Cache is faulty or inaccessible, or the disk storage is unreachable from the host, the Flash Read Cache instances on that host are invalid and do not work to accelerate I/Os. As a result, the caches do not serve stale data after connectivity is re-established between the host and storage. The connectivity outage might be temporary, all paths down (APD) condition, or permanent, permanent device loss (PDL). This condition persists until the virtual machine is power-cycled.
</p>
<p>Workaround: The virtual machine can be power-cycled to restore I/O acceleration using Flash Read Cache.</p>
</li>



<!-- DOCNOTES: GA 1374488 GG-->
<li><p><strong>All Paths Down (APD) or path-failovers might cause system failure 
 </strong><br />
In a shared SAS environment, APD or path-failover situations might cause system failure if the disks are claimed by the lsi_msgpt3 driver and they are experiencing heavy I/O activity.
</p>

<p>Workaround: None  </p>
</li>



<!-- DOCNOTES: GA 1379666 GG-->
<li><p><strong>Frequent use of SCSI abort commands can cause system failure </strong><br />

With heavy I/O activity, frequent SCSI abort commands can cause a very slow response from the MegaRAID controller. If an unexpected interrupt occurs with resource references that were already released in a previous context, system failure might result.
</p>
<p>Workaround: None  </p>
</li>



<!-- DOCNOTES: GA 1338161 SV -->
<li><p><strong>iSCSI connections fail and datastores become inaccessible when IQN changes</strong><br />
This problem might occur if you change the IQN of an iSCSI adapter while iSCSI sessions on the adapter are still active. </p>
<p>Workaround: When you change the IQN of an iSCSI adapter, no session should be active on that adapter. Remove all iSCSI sessions and all targets on the adapter before changing the IQN.</p>
</li>




<!-- DOCNOTES: GA 1336704 SV -->
<li><p><strong>nvmecli online and offline operations might not always take effect </strong><br />
When you perform the <code>nvmecli device online -A vmhba*</code> operation to bring a NVMe device online, the operation appears to be successful. However, the device might still remain in offline state.
</p>
<p>Workaround: Check the status of NVMe devices by running the <code>nvmecli device list</code> command.</p>
</li>
</ul>

<!-- <b><a name="backupissues"></a>Backup Issues</b> -->


<b><a name="serverissues"></a>Server Configuration Issues</b>
<ul>
<!-- DOCNOTES: 1350993-->
<li><p><strong>Remediation fails when applying a host profile from a stateful host to a host provisioned with Auto Deploy</strong><br />
When applying a host profile from a statefully deployed host to a host provisioned with Auto Deploy (stateless host) with no local 
storage, the remediation attempt fails with one of the following error messages:</p>
<ul>
<li><p><code>The vmhba device at PCI bus address <em>sxxxxxxxx.xx</em> is not present on your host. You must shut down and then insert a card into PCI slot <em>yy</em>. 
The type of card should exactly match the one in the reference host.</code></p></li>
<li><p><code>No valid coredump partition found.</code></p></li>
</ul>
</p>
<p>Workaround: Disable the plug-in that is causing the issue (for example, the Device Alias Configuration or Core Dump Configuration) 
from the host profile, and then remediate the host profile.</p>
</li>

<!-- DOCNOTES: 1343252-->
<li><p><strong>Applying host profile with static IP to a host results in compliance error</strong><br />
If you extract a host profile from a host with a DHCP network configuration, and then edit the host profile to have
a static IP address, a compliance error occurs with the following message when you 
apply it to another host:</p>
<p>
<code>Number of IPv4 routes did not match.</code>
</p>
<p>Workaround: Before extracting the host profile from the DHCP host, configure the host so that it has a static IP 
address.</p>
</li>

<!-- DOCNOTES: 1385992 -->
<li><strong>When you hot-add a virtual network adapter that has network resources overcommitted, the virtual machine might be powered off</strong><br />
On a vSphere Distributed Switch that has Network I/O Control enabled, a powered on virtual machine is configured with a bandwidth reservation   according to the reservation for virtual machine system traffic on the physical network adapter on the host. You hot-add a network adapter to   the virtual machine setting network bandwidth reservation that is over   the bandwidth available on the physical network adapters on the host.
<p>When you hot-add the network adapter, the VMkernel starts a Fast Suspend and Resume (FSR) process. Because the virtual machine requests more network resources than available, the VMkernel exercises the failure path of the FSR process. A fault in this failure path causes the virtual machine to power off.</p>

  <p>Workaround: Do not configure bandwidth reservation when you add a network adapter to a powered on virtual machine.</p>
</li>
</ul>

<!--<b><a name="clientissues"></a>vCenter Server, vSphere Web Client, and vSphere Client Issues</b><br>-->






<!-- <b><a name="vmissues"></a>Virtual Machine Management Issues</b> -->




<!-- <b><a name="migrationissues"></a>Migration Issues</b><br> -->



 <b><a name="vmwarehaissues"></a>VMware HA and Fault Tolerance Issues</b>

<ul>

<!-- DOCNOTES: GA 1505025-->
<li><p><strong>Legacy Fault Tolerance (FT) not supported on Intel Skylake-DT/S, Broadwell-EP, Broadwell-DT, and Broadwell-DE platform
</strong><br />
Legacy FT is not supported on Intel Skylake-DT/S, Broadwell-EP, Broadwell-DT, and Broadwell-DE platform. Attempts to power on a virtual machine will fail after you enable single-processor, Legacy Fault Tolerance.</p> 
<p>Workaround: None.</p>
</li>

</ul> 

<b><a name="gosissues"></a>Guest Operating System Issues</b><br> 

<ul>

<!-- DOCNOTES: GA 1377102-->
<li><p><strong>Attempts to enable passthrough mode on NVMe PCIe SSD devices might fail after hot plug</strong><br />
To enable passthrough mode on an SSD device from the vSphere Web Client, you select a host, click 
the <strong>Manage</strong> tab, click <strong>Settings</strong>, navigate to the <strong>Hardware</strong> section,
click <strong>PCI Devices</strong> &gt; <strong>Edit</strong>, select a device from a list of active devices that can be 
enabled for passthrough, and click <strong>OK</strong>. However, when you hot plug a new NVMe device to an ESXi 6.0 host that does not have a 
PCIe NVMe drive, the new NVMe PCIe SSD device cannot be enabled for passthrough mode and does not appear in the list of 
available passthrough devices.</p> 
<p>Workaround: Restart your host. You can also run the command on your ESXi host.</p>
<ol>
<li><p>Log in as a root user.</p></li>
<li><p>Run the command <br /> <tt>/etc/init.d/hostd start</tt></p></li>
</ol>

</li>

</ul> 


<b><a name="hardwareissues"></a>Supported Hardware Issues</b> 

<ul>

<!-- DOCNOTES: 1411638 -->
<li><p><strong>When you run esxcli to get the disk location, the result is not correct for Avago controllers on HP servers</strong><br />
<p>When you run <code>esxcli storage core device physical get</code>, against an Avago controller on an HP server, the result is not correct.</p>

<p>For example, if you run:<br />
<code>esxcli storage core device physical get -d naa.5000c5004d1a0e76</code><br />
The system returns:<br />
   <code>Physical Location: enclosure 0, slot 0</code>
</p>

<p>The actual label of that slot on the physical server is 1.
</p>
<p>Workaround: Check the slot on your HP server carefully. Because the slot numbers on the HP server start at 1, you have to increase the slot number that the command returns for the correct result. </p>
</li>
</ul>



<!-- <b><a name="i18nissues"></a>Internationalization Issues</b> -->



<!-- <b><a name="vicli_issues"></a>vSphere Command-Line Interface Issues</b> -->



<!--<b><a name="adib_issues"></a>Auto Deploy and Image Builder</b>-->



<b><a name="cimapiissues"></a>CIM and API Issues</b>

<ul>
<!-- DOCNOTES: GA 1504983 -->
<li><p><strong>The sfcb-vmware_raw might fail</strong><br />
The sfcb-vmware_raw might fail as the maximum default plugin resource group memory allocated is not enough.</p>
<p>Workaround: Add UserVars <tt>CIMOemPluginsRPMemMax</tt> for memory limits of sfcbd plugins using the following command and restart the sfcbd for the new plugins value to take effect:<br><br>
<tt>esxcfg-advcfg -A CIMOemPluginsRPMemMax --add-desc 'Maximum Memory for plugins RP' --add-default XXX --add-type int --add-min 175 --add-max 500</tt><br><br>
XXX being the memory limit you want to allocate. This value should be within the minimum (175) and maximum (500) values.
</p>
</li>
</ul>
<!--<b><a name="miscissues"></a>Miscellaneous Issues</b>-->


<!--<b><a name="vmtoolsissues"></a>VMware Tools Issues</b>-->






<!-- ///*** end of content area ***/// --><!-- ///*** Do not modify anything after this comment ***/// --></td></tr></table></div> <!-- content --></div>  <!-- content-container --><div id="ifooter"></div><script type="text/javascript" src="/Release_Notes/scripts/techpub_relnote.js"></script></div>  <!-- container --></body></html>


