<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><!-- InstanceBegin template="/docroot/Templates/GammaArchiveStyle.dwt.php" codeOutsideHTMLIsLocked="true" -->
<head>

<meta http-equiv="Content-Type" content="text/html; charset=charset=utf-8" />
<!-- InstanceBeginEditable name="doctitle" -->

<meta name="ProgId" content="FrontPage.Editor.Document" />

<meta name="Description" content="This paper, published in 1989, describes the Brewer-Nash Chinese Wall Security Policy" />
<meta name="Keywords" content="Chinese Wall, security policy, security policy model, formal security policy model" />
<!-- InstanceEndEditable --><!-- InstanceBeginEditable name="head" --><title>The Chinese Wall security policy</title>
<!-- InstanceEndEditable -->
<script type="text/JavaScript">
<!--
function MM_openBrWindow(theURL,winName,features) { //v2.0
  window.open(theURL,winName,features);
}
//-->
</script>
<meta name="Keywords" content ="corporate governance, internal control, effective internal control, sound internal control, measuring the effectiveness of internal control, Brewer, Brewer and List, time theory, risk treatment plan, risk assessment, opportunity assessment, AIL, RTP, opportunity exploitation plan, OEP, integrated management system, management system, ISO 9000, ISO 9001, ISO9001, QMS, ISO/IEC 27001, ISO 27001, ISO27001, ISMS, BS25999, BS 25999, BCMS, ISO 14000, ISO 20000, ISO/IEC 20000, ITSMS, OHSMS 18000, OHSMS, FSMS, ISO 22000, ISO22000, PAS 99" />
<meta name="Keywords" content="ISMS, ISO Guide 83, Guide 83, 83, IMS-Smart, IMS, Annex SL" />
<meta name="Keywords" content="ISO/IEC 27001, ISO 27001, 27001, ISO/IEC 27011, ISO 27011, 27011, " />
<meta name="Keywords" content="ISO/IEC 27002, ISO 27002, 27002, ISO/IEC 27012, ISO 27012, 27012" />
<meta name="Keywords" content="ISO/IEC 27002, ISO 27002, 27002, ISO/IEC 27012, ISO 27012, 27012" />
<meta name="Keywords" content="ISO/IEC 27003, ISO 27003, 27003, ISO/IEC 27013, ISO 27013, 27013" />
<meta name="Keywords" content="ISO/IEC 27004, ISO 27004, 27004, ISO/IEC 27014, ISO 27014, 27014" />
<meta name="Keywords" content="ISO/IEC 27005, ISO 27005, 27005, ISO/IEC 27015, ISO 27015, 27015" />
<meta name="Keywords" content="ISO/IEC 27006, ISO 27006, 27006, ISO/IEC 27016, ISO 27016, 27016" />
<meta name="Keywords" content="ISO/IEC 27007, ISO 27007, 27007, ISO/IEC 27017, ISO 27017, 27017" />
<meta name="Keywords" content="ISO/IEC 27008, ISO 27008, 27008, ISO/IEC 27018, ISO 27018, 27018" />
<meta name="Keywords" content="ISO/IEC 27009, ISO 27009, 27009, ISO/IEC 27019, ISO 27019, 27019" />
<meta name="Description" content="Experts in information security, specialising in ISO/IEC 27001 and integrated management systems" />
<link rel="SHORTCUT ICON" href="../favicon.ico" />
<link href="../style/GammaStyle2.css" rel="stylesheet" type="text/css" />
<style type="text/css">
<!--
.style3 {
	font-size: 9px;
	color: #FFFFFF;
	line-height: 10px;
	font-weight: bold;
}
-->
</style>
<!-- InstanceParam name="gammaCopyRight" type="boolean" value="true" -->
</head>
<body>
<table width="1011" border="0" align="center" cellpadding="0" cellspacing="0">
  <tr>
    <td width="9%" rowspan="3"><img src="../style/logo_sm6.gif" title="Gamma logo - a red sun with green leaves, inspired on seeing the painting &#8220;Surprise!&#8221; (1891) by Henri Rousseau. Gamma&#8217;s strap line at the time was &#8220;The IT security jungle is a dangerous place &#8212; Gamma: your guide through the jungle&#8221;" width="166" height="70" /></td>
    <td width="86%"><div align="right" class="banner"><em>Gamma Secure Systems Limited</em>&nbsp;&nbsp;</div></td>
  </tr>
  <tr>
    <td><div align="right"><span class="small">SPECIALISTS IN  INFORMATION SECURITY MANAGEMENT SYSTEMS (ISO/IEC 27001)</span>&nbsp;&nbsp;</div></td>
  </tr>
  <tr><td colspan="6" bgcolor="#005A52" class="White"><div align="center">YOU ARE IN GAMMA&#8217;S RESEARCH ARCHIVES &#8212; THIS PAGE IS OF HISTORIC INTEREST ONLY &#8212; <a href="archives.php" class="White" Title="Exit to research archives index page. You can use the menu bar from here to access the whole Gamma web site.">EXIT</a></div></td>
  </tr>
</table>

<p>&nbsp;</p>

  
<table width="1011" border="0" align="center" cellspacing="0" cellpadding="0">
  <tr>
    <td width="8" bgcolor="#005A52"></td>
    <td width="16"></td>
    <td colspan="2"><!-- InstanceBeginEditable name="Text" --><p></p>
<p class="H1">THE CHINESE WALL SECURITY POLICY*</p>
<p><i>by Dr. David F. C. Brewer and Dr. Michael J. Nash</i></p>
<table border="4" bgcolor="#AD3942">
  <tr>
    <td bgcolor="#AD3942"><font color="#FFFFFF" size="1">* PUBLISHED AT THE IEEE SYMPOSIUM ON RESEARCH IN SECURITY AND PRIVACY, 1-3
      MAY 1989, OAKLAND, CALIFORNIA. (pp 206-14)&nbsp; &copy; 1989 IEEE. Personal use of
      this material is permitted. However, permission to reprint/republish this
      material for advertising or promotional purposes or for creating new collective
      works for resale or redistribution to servers or lists, or to reuse any
      copyrighted component of this work in other works must be obtained from the
      IEEE.</font></td>
  </tr>
</table>
<p align="justify"><font size="2"> <a href="chwall.pdf" target="_blank">Click here</a> to download a version
  in pdf format </font> </p>
<p class="H2">Abstract</p>
<p>Everyone who has seen the movie Wall Street will have seen a commercial
  security policy in action. The recent work of Clark and Wilson and the WIPCIS
  initiative (the Workshop on Integrity Policy for Computer Information Systems)
  has drawn attention to the existence of a wide range of commercial security
  policies which are both significantly different from each other and quite alien
  to current &quot;military&quot; thinking as implemented in products for the
  security market place.</p>
<p>This paper presents a basic mathematical theory which implements one such
  policy, the Chinese Wall, and shows that it cannot be correctly represented by a
  Bell-LaPadula model.</p>
<p>The Chinese Wall policy combines commercial discretion with legally
  enforceable mandatory controls. It is required in the operation of many
  financial services organizations and is, therefore, perhaps as significant to
  the financial world as Bell-LaPadula's policies are to the military.<br />
                <br />
</p>
<p class="H2">INTRODUCTION</p>
<p>Until recently, military security policy thinking has dominated the direction
  of computer security research both in the US and the UK. Clark and Wilson's
  seminal paper [1] has, however, drawn attention to the fact that commercial
  security needs are just as important as those of the defence community and,
  through the WIPCIS initiative [2], that the problems of the commercial community
  are at least as diverse and relevant to the computer scientist.</p>
<p>There are many well defined commercial security policies covering all aspects
  of Clark and Wilson's model [3]. One of these, the <i>Chinese Wall</i> security
  policy is perhaps as significant to some parts of the commercial world as Bell
  and LaPadula's policies [4, 5] are to the military. It can be most easily
  visualized as the code of practice that must be followed by a market analyst
  working for a financial institution providing corporate business services. Such
  an analyst must uphold the confidentiality of information provided to him by his
  firm's clients; this means he cannot advise corporations where he has <i>insider
    knowledge</i> of the plans, status or standing of a competitor. However, the
  analyst is free to advise corporations which are not in competition with each
  other, and also to draw on general market information. Many other instances of
  Chinese Walls are found in the financial world.</p>
<p>Unlike Bell and LaPadula, access to data is not constrained by attributes of
  the data in question but by what data the subject already holds access rights
  to. Essentially, datasets are grouped into &quot;conflict of interest
  classes&quot; and by mandatory ruling all subjects are allowed access to at most
  one dataset belonging to each such conflict of interest class; the actual choice
  of dataset is totally unrestrained provided that this mandatory rule is
  satisfied. We assert that such policies cannot be correctly modelled by Bell-LaPadula.</p>
<p>It should be noted that in the United Kingdom the Chinese Wall requirements
  of the UK Stock Exchange [6] have the authority of law [7] and thus represent a
  mandatory security policy whether implemented by manual or automated means.</p>
<p>Furthermore, correct implementation of this policy is important to English
  Financial Institutions since it provides a legitimate defence against certain
  penal classes of offence under their law.<br />
                <br />
</p>
<p class="H2">CHINESE WALLS</p>
<p class="H3">A Model</p>
<p>It is useful to devise a model of the Chinese Wall policy, not only to
  facilitate sound reasoning of its properties but also to permit comparison with
  models of other policies.</p>
<p>Since, in particular, we wish to compare it with the Bell-LaPadula (BLP)
  model we will adopt the latter's concepts of <i>subjects, objects and security
    labels</i>. Our model is then developed by first defining what we mean by a <i>Chinese
      Wall</i> and then, by devising a set of rules such that no person (subject) can
  ever access data (objects) on the <i>wrong side</i> of that wall.</p>
<p><br />
  <span class="H3">Database organization</span></p>
<p>We start by informally defining the concept of a Chinese Wall and, in
  particular, what is meant by being on the <i>wrong</i> side of it. A formal
  specification will be found in <a href="#ANNEX A">Annex A</a>.</p>
<p>All corporate information is stored in a hierarchically arranged filing
  system such as that shown in figure 1. There are three levels of significance:</p>
<ol>
  <li>
    <p>at the lowest level, we consider individual
      items of information, each concerning a single corporation. In keeping with
      BLP, we will refer to the files in which such information is stored as <i>objects</i>; </p>
  </li>
  <li>
    <p>at the intermediate level, we group all objects
      which concern the same corporation together into what we will call a <i>company
        dataset</i>; </p>
  </li>
  <li>
    <p>at the highest level, we group together all
      company datasets whose corporations are in competition. We will refer to
      each such group as a <i>conflict of interest class</i>. </p>
  </li>
</ol>
<p>Associated with each object is the name of the company dataset to which it
  belongs and the name of the conflict of interest class to which that company
  dataset belongs. For example, the conflict of interest class names could be the
  business sector headings found in stock exchange listings (Petrochemical,
  Financial Services, etc.); the company dataset names could be the names of
  companies listed under those headings. For convenience let us call the <i>r</i>-th
  object <i>o</i><sub>r</sub> and refer to its company dataset as <i>y</i><sub>r</sub> and its conflict of interest class as <i>x</i><sub>r</sub>.<br />
</p>
<p>Thus, if our system maintained information say on Bank-A, Oil Company-A and
  Oil Company-B:</p>
<ol>
  <li>
    <p>all objects would belong to one of three
      company datasets (i.e. <i>y</i><sub>r</sub> would be &quot;Bank-A&quot;,
      &quot;Oil Company-A&quot; or &quot;Oil Company-B&quot;) and </p>
  </li>
  <li>
    <p>there would be two conflict of interest
      classes, one for banks (containing Bank-A's dataset) and one for petroleum
      companies (containing Oil Company-A's and Oil Company-B's datasets), i.e. <i>x</i><sub>r</sub> would be &quot;banks&quot; or &quot;petroleum companies&quot; </p>
  </li>
</ol>
<p>This structure is of great importance to the model and is represented in the
  annex as an axiom (A1).<br />
</p>
<p class="H3">Simple security</p>
<p>The basis of the Chinese Wall policy is that people are only allowed access
  to information which is not held to conflict with any other information that
  they already possess. As far as the computer system is concerned the only
  information <i>already possessed</i> by a user must be information that:</p>
<ol>
  <li>
    <p>is held on the computer, and </p>
  </li>
  <li>
    <p>that user has previously accessed. </p>
  </li>
</ol>
<p>Thus, in consideration of the Bank-A, Oil Company-A and Oil Company-B
  datasets mentioned previously, a <i>new</i> user may freely choose to access
  whatever datasets he likes; as far as the computer is concerned a new user does
  not possess <i>any</i> information and therefore no conflict can exist. Sometime
  later, however, such a conflict may exist.</p>
<p>Suppose our user accesses the Oil Company-A dataset first; we say that our
  user now <i>possesses</i> information concerning the Oil Company-A dataset.
  Sometime later he requests access to the Bank-A dataset. This is quite
  permissible since the Bank-A and Oil Company-A datasets belong to different
  conflict of interest classes and therefore no conflict exists. However, if he
  requests access to the Oil Company-B dataset the request must be denied since a
  conflict does exist between the requested dataset (Oil Company-B) and one
  already possessed (Oil Company-A).</p>
<p align="left"><img border="0" src="archives/images/chinesewall_fig1.gif" width="414" height="316" title="Figure 1 - The composition of objects"/><br />
    <b>Figure 1 - The composition of objects</b></p>
<p>We note that it does not matter whether the Oil Company-A dataset was
  accessed before or after the Bank-A dataset. However, were Oil Company-B to be
  accessed before the request to access the Oil Company-A dataset, the
  restrictions would be quite different. In this case access to the Oil Company-A
  dataset would be denied and our user would possess {&quot;Oil Company-B&quot;,
  &quot;Bank-A&quot;} (as opposed to the request to access the Oil Company-B
  dataset being denied and the user possessing {&quot;Oil Company-A&quot;,
  &quot;Bank-A&quot;}).</p>
<p>What we have just described is a Chinese Wall. We note, in the first
  instance, that our user has complete freedom to access anything he cares to
  choose. Once that initial choice has been made, however, a <i>Chinese Wall</i> is created for that user around that dataset and we can think of &quot;the <i>wrong</i> side of this Wall&quot; as being any dataset within the same conflict of
  interest class as that dataset <i>within</i> the Wall. Nevertheless, the user
  still has freedom to access any other dataset which is in a different conflict
  of interest class, but as soon as that choice is made, the Wall <i>changes shape</i> to include the new dataset. Thus we see that the Chinese Wall policy is a subtle
  combination of free choice and mandatory control.</p>
<p>More formally, we can introduce BLP's concept of a subject to represent a
  user, and indeed any program that might act on his behalf. Again, it is
  convenient to be able to identify any particular <i>subject</i>, let us call it <i>s</i><sub>u</sub> for the <i>u</i>-th subject. However, we conclude that it is necessary for the
  computer to remember who has accessed what. Annex A introduces a formalized
  device (D1), <b><u>N</u></b>, to do this. Essentially <b><u>N</u></b> is a
  matrix with a column for every object in the system and a row for every subject;
  any particular cell therefore corresponds to a particular (subject, object)
  pair. <b><u>N</u></b> simply records who has accessed what, in other words who <i>possesses</i> what. For example if a subject, <i>s</i><sub>u</sub>, has accessed some object
  or then <b><u>N</u></b>(u, r) would be set true, otherwise it would be set
  false. Once some request by, say, <i>s</i><sub>u</sub>, to access some new
  object, say <i>o</i><sub>t</sub>, is granted then <b><u>N</u></b>, of course,
  must be updated; in particular <b><u>N</u></b> must be replaced by some new <b><u>N</u></b>'
  in which <b><u>N</u></b>'(u, t) takes the value true.</p>
<p>Given this we can then define a security rule (axiom A2 in Annex A). This
  rule means that:</p>
<blockquote>
  <p><b>Access is only granted if the object requested:</b></p>
  <ol>
    <li>
      <p><b>is in the same company dataset as an
        object already accessed by that subject, i.e. within the Wall, or</b> </p>
    </li>
    <li>
      <p align="left"><b>belongs to an entirely
        different conflict of interest class.</b></p>
    </li>
  </ol>
</blockquote>
<p align="right"><b><br />
  (A2)</b>.</p>
<p>This rule is, however, insufficient in itself; it is necessary to ensure that
  initially, i.e. before any subject has requested access to any object, that <b><u>N</u></b> has been correctly initialized (everywhere false). Moreover it is necessary to
  formally define the notion that, given such an initial state, then the first
  ever access will be granted. These two requirements are stated as axioms A3 and
  A4 (respectively) in Annex A.</p>
<p>This rule is analogous to the <i>simple</i> security rule defined in the BLP
  model and since these two rules will be compared with each other, we will call
  our rule (A2) the <i>simple</i> security rule as well. (Not surprisingly, we
  will define a *-property rule as well, see later.)<br />
</p>
<p class="H3">Accessibility</p>
<p>The basic formulation of the Chinese Wall policy is expressed in Annex A as
  axioms A1-A4. Given these, we can prove some useful theorems about the policy.
  Annex A does this. The first two theorems merely confirm that our informal
  concepts of a Chinese Wall are indeed implied by these rules, in particular:</p>
<blockquote>
  <p><b>T1) Once a subject has accessed an object the only other objects
    accessible by that subject lie within the same company dataset or within a
    different conflict of interest class.</b></p>
  <p><b>T2) A subject can at most have access to one company dataset in each
    conflict of interest class.</b></p>
  <p>A third theorem concerns a most pragmatic consequence of a Chinese Wall
    policy, and that is <i>how many people do you need to operate one</i>?:<br />
                    <br />
                    <b>T3) If for some conflict of interest class <i>X</i> there are <i>X</i><sub>Y</sub> company datasets then the minimum number of subjects which will allow every
                      object to be accessed by at least one subject is <i>X</i><sub>Y</sub>.</b></p>
</blockquote>
<p>This tells us that it is the conflict of interest class with the largest
  number of member company datasets (L, say) which will give us the answer.
  However, because of the free choice nature of the policy, should two or more
  users choose to access the same dataset then this number must be increased.
  Moreover, if we had an Automobile class with 5 motor companies (L<sub>Y</sub> =
  5) and five users each with access to <i>different</i> motor companies, but <i>all</i> choose to access the <i>same</i> petroleum company, Oil Company-A, then who can
  access Oil Company-B?</p>
<p>Hence, in practice, it is not necessarily the largest conflict of interest
  class that gives us an accessibility problem. L<sub>Y</sub> is nevertheless the
  minimum number of subjects ever required and thus the minimum number of analysts
  the finance house must employ.</p>
<p class="H3">Sanitized information</p>
<p>It is usual in the application of the Chinese Wall policy for company
  datasets to contain sensitive data relating to individual corporations, the
  conflict of interest classes being in general business sectors. However it is
  considered important to be able to generally compare such information with that
  relating to other corporations. Clearly a problem arises if access to any
  corporation is prohibited by the simple security rule due to some previous
  access (a user can never compare Oil Company-A's data with Oil Company B's data,
  nor can he compare Bank-A's data with Oil Company-A's data if he has had
  previous access to Oil Company-B's data). This restriction can, however, be
  removed if we use a sanitized form of the information required.</p>
<p>Sanitization takes the form of disguising a corporation's information, in
  particular to prevent the discovery of that corporation's identity. Obviously,
  effective sanitization cannot work unless there is sufficient data to prevent
  backward inference of origin. Since it is not the objective of this paper to
  discuss the problem of inference we will assume that sanitization is possible.
  In practice, this has been found to be the case [3].</p>
<p>It seems sensible, therefore, to regard all company datasets, bar one, as
  containing sensitive information belonging to some particular corporation. We
  reserve the remaining dataset, <i>y</i><sub>o</sub> (say), for sanitized
  information relating to all corporations (D2 in Annex A).</p>
<p>It is unnecessary to restrict access to such sanitized information. This can
  be accomplished without need to rework any of the preceding theory by asserting
  that this distinguished company dataset, <i>y</i><sub>o</sub>, is the sole
  member of some distinguished conflict of interest class, <i>x</i><sub>o</sub> (A5). In other words, if an object bears the security label <i>y</i><sub>o</sub> then it must also bear the label <i>x</i><sub>o</sub> and vice versa. T2 tells
  us that all subjects can access this company dataset.</p>
<h3><br />
  *-Property</h3>
<p>Suppose two subjects, User-A and User-B, have between them access to the
  three datasets, Oil Company-A, Oil Company-B and Bank-A, in particular User-A
  has access to Oil Company-A and Bank-A and User-B has access to Oil Company-B
  and Bank-A. If User-A reads information from Oil Company-A and writes it to
  Bank-A then User-B can read Oil Company-A information. This should not be
  permitted, however, because of the conflict of interest between Oil Company-A
  and Oil Company-B. Thus indirect violations of the Chinese Wall policy are
  possible.</p>
<p>We can prevent such violations by insisting that:</p>
<blockquote>
  <p><b>Write access is only permitted if</b></p>
  <ol>
    <li>
      <p><b>access is permitted by the simple security
        rule, and</b> </p>
    </li>
    <li>
      <p align="left"><b>no object can be read which
        is in a different company dataset to the one for which write access is
        requested and contains unsanitized information.</b><br />
      </p>
    </li>
  </ol>
  <p align="right"><b>(A6)</b>.</p>
</blockquote>
<p>Given this rule we can prove that:</p>
<blockquote>
  <p><b>T4) The flow of unsanitized information is confined to its own company
    dataset; sanitized information may however flow freely throughout the system.</b></p>
</blockquote>
<p>The rule is analogous to the <i>*-property</i> security rule defined in the
  BLP model; we will therefore call our rule (A6) the <i>*-property</i> security
  rule as well.</p>
<p class="H2">BELL-LaPADULA</p>
<p>It is instructive to compare the Chinese Wall policy with that of the Bell-LaPadula
  model [4, 5]. Both policies are described in similar terms: the composition and
  security attributes of the objects concerned and the rules for access, both in
  respect of simple-security and *-property.</p>
<p>We take each of these topics in turn and identify a transformation which
  permits the Bell-LaPadula model (BLP) to describe that aspect of the Chinese
  Wall policy as faithfully as possible.<br />
  Object composition</p>
<p>The BLP model places no constraints upon the interrelationships between
  objects, in particular it does not require them to be hierarchically arranged
  into company datasets and conflict of interest classes. Instead it imposes a
  structure upon the security attributes themselves.</p>
<p>Each object within the BLP model [4] has a security label of the form (<i>class,</i> {<i>cat </i>}) where</p>
<ol>
  <li>
    <p><i>class</i> is the classification of the
      information (e.g. unclassified, confidential, secret) </p>
  </li>
  <li>
    <p><i>cat</i> is the formal category of the
      information (e.g. NOFOR). </p>
  </li>
</ol>
<p>Unlike the Chinese Wall policy, BLP attaches security attributes to subjects
  as well. They are complementary to object labels and have the form (<i>clear</i>, <i>NTK</i>) where:</p>
<ol>
  <li>
    <p><i>clear</i> is the subject's clearance, i.e.
      the maximum classification to which he is permitted access </p>
  </li>
  <li>
    <p><i>NTK</i> is the subject's need-to-know, i.e.
      the sum total of all categories to which he is permitted access. </p>
  </li>
</ol>
<p>Although this labelling scheme is seemingly quite different from that of
  figure 1, the two object compositions can be brought together quite simply by:</p>
<ol>
  <li>
    <p>reserving one BLP classification for sanitized
      information and another for unsanitized information </p>
  </li>
  <li>
    <p>assigning each Chinese Wall (<i>x, y</i>) label
      to a unique BLP category </p>
  </li>
  <li>labelling each object containing unsanitized information with the BLP
    category corresponding to its (<i>x, y</i>) label and the BLP category of
    the sanitized dataset (<i>x</i><sub>o</sub>, <i>y</i><sub>o</sub>).</li>
</ol>
<p>The result of applying this transformation to the object composition of
  figure 1 is shown in figure 2.<br />
                <img border="0" src="archives/images/chinesewall_fig2.gif" width="321" height="322" title="Figure 2 Composition of objects into BLP-categories"/></p>
<p><b>Figure 2 Composition of objects into BLP-categories</b><br />
</p>
<p class="H3">Access rules</p>
<p>BLP also has a simple-security rule and a *-property rule:</p>
<ol>
  <li>
    <p><u><b>Simple security</b></u>: access is
      granted only if the subject's clearance is <i>greater</i> than the object's
      classification and the subject's need-to-know <i>includes</i> the object's
      category(ies) </p>
  </li>
  <li>
    <p><u><b>*-property</b></u>: write access is
      granted only if the output object's classification is <i>greater</i> than
      the classification of all input objects, and its category <i>includes</i> the category(ies) of all input objects.<br />
    </p>
  </li>
</ol>
<p>We note that, in contrast to the Chinese Wall policy, these rules do not
  constrain access by the objects already possessed by a subject, but by the
  security attributes of the objects in question. Even so, a transformation does
  exist:</p>
<ol>
  <li>
    <p>give all subjects access to both sanitized and
      unsanitized objects </p>
  </li>
  <li>
    <p>define a need-to-know category for all possible
      combinations of object category which satisfy T2 </p>
  </li>
  <li>
    <p>assign one such need-to-know category to each
      subject at the outset. </p>
  </li>
</ol>
<p>Referring to figure 2, for example, we would have 27 combinations:</p>
<blockquote>
  <blockquote>
    <blockquote>
      <blockquote>
        <blockquote>
          <blockquote>
            <p>{0, 1, 2, 3},</p>
            <p>{0, 1, 2, 6},</p>
            <p>{0, 1, 2, 9},</p>
            <p>{0, 1, 5, 3}, ....</p>
            <p>....,{0, 7, 8, 9}</p>
          </blockquote>
        </blockquote>
      </blockquote>
    </blockquote>
  </blockquote>
</blockquote>
<p>We then assign one of these to each subject, e.g. if we had three subjects,
  User-A, User-B and User-C, we could assign them:</p>
<blockquote>
  <blockquote>
    <blockquote>
      <blockquote>
        <blockquote>
          <p>User-A has {0, 1, 2, 3}</p>
          <p>User-B has {0, 4, 8, 6}</p>
          <p>User-C has {0, 7, 5, 9}</p>
        </blockquote>
      </blockquote>
    </blockquote>
  </blockquote>
</blockquote>
<p>Now, the BLP simple security rule will work as we have granted subjects
  need-to-know combinations of company datasets for which no conflict of interest
  exists. BLP *-property also works. The category of the input objects must be
  included in the category of the output objects; input objects must therefore
  either contain sanitized information or be of the same company dataset as the
  output object. Moreover, unsanitized information dominates sanitized
  information.<br />
</p>
<p class="H3">Conclusion</p>
<p>These transformations give good account of BLP's ability to model to Chinese
  Wall security policy; they are not, however, entirely satisfactory.</p>
<p>For example, suppose in the real world management suddenly required User-A to
  have access to company dataset-8 because User-B was taken sick; what then? We
  cannot just change User-A's need-to-know to {0, 1, 8, 3} unless we know for
  certain that he has not yet accessed company dataset-2; otherwise we would<br />
  violate the Chinese Wall. The BLP model does not provide the necessary
  information to answer this question.</p>
<p>Secondly, BLP only works if subjects are not given the freedom to choose
  which company datasets they wish to access. In other words, these
  transformations totally ignore the free choice nature of the Chinese Wall
  policy. This freedom of choice can be restored (e.g. by extending subject
  need-to-know to cover all company datasets) but only at the expense of failing
  to express the mandatory controls.</p>
<p>Thus, we can use BLP to model either the mandatory part or the free choice
  part of the Chinese Wall policy but not both at the same time. This is a
  somewhat fundamental restriction as the Chinese Wall policy requires both! The
  Chinese Wall model must therefore be regarded as distinct from BLP and important
  in its own right.<br />
</p>
<p class="H2">CLARK AND WILSON</p>
<p>It is interesting to consider the Chinese Wall policy in the light of Clark
  and Wilson's work. The Clark-Wilson model [1] defines a set of rules, based on
  commercial data processing practices, which together have the objective of
  maintaining data integrity. One such rule (E2), observed in many commercial
  systems [3], describes a finer degree of access control than that required by
  BLP. Compatibility of the Chinese Wall model with E2 is important because of the
  practical significance of that rule; it would not be useful if in order to
  satisfy E2 a system could not also satisfy the Chinese Wall policy.</p>
<p>Rule E2 requires access to be controlled on the basis of a set of relations
  of the form (user, program, object) which relates a user, a program, and the
  objects that program may reference on behalf of that user. Of greatest interest
  is the observation that if all users are permitted access to a given process,
  which in turn is allowed to access any object, then it does not follow that all
  users are permitted access to all objects. This may apply in the case of a
  Chinese Wall policy where, for example, some users may have access to a
  spreadsheet package but may not be permitted to use it on all company datasets
  to which they are permitted access.<br />
</p>
<p class="H3">A refined exposition of the Chinese Wall policy</p>
<p>The Chinese Wall policy as described so far refers to the users of the
  system. Thus we may interpret the use of the word <i>subject</i> in the various
  axioms, definitions and theorems as users. To be more explicit we define user as
  a <i>human being who causes or attempts to cause a transaction in a system</i>.
  In particular it is incorrect to associate <i>subject</i> with a (<i>user,
    process</i>) pair since, A2 for example, would imply that any particular user
  could gain access to two datasets within the same conflict of interest class
  simply by accessing them via different processes.</p>
<p>We may then accommodate Clark and Wilson's observation by simply requiring
  that:</p>
<ol>
  <li>
    <p><b>users may only execute certain named
      processes (A7)</b> </p>
  </li>
  <li>
    <p><b>those processes may only access certain
      objects (A8).</b> </p>
  </li>
</ol>
<p>Axiom A8 is defined in such a way as to maintain independence between what a
  user is allowed to do and what a process is allowed to do. This allows Clark and
  Wilson's observation to be incorporated into our model:</p>
<ol>
  <li>
    <p>without change to any of the foregoing theory
      (in particular we simply restate all axioms, definitions and theorems by
      replacing <i>subject</i> with <i>user</i>) </p>
  </li>
  <li>
    <p>redefining the overall <i>simple</i> and <i>*-property</i> security rules to be the logical conjunction of all user-object,
      user-process and process-object permissions. </p>
  </li>
</ol>
<p>Thus a process is allowed to access an object if the user requesting it is
  allowed to execute it <i>and</i> that user, by the Chinese Wall policy, is
  allowed to access that object and that process is also allowed to access that
  object; write access is then permitted only if the <i>*-property</i> rule (A6)
  is satisfied. Thus we conclude that the Chinese Wall model is not at variance
  with the observations of Clark and Wilson.<br />
</p>
<p class="H2">OVERALL CONCLUSIONS</p>
<p>In this paper we have explored a commercial security policy which represents
  the behaviour required of those persons who perform corporate analysis for
  financial institutions. It can be distinguished from Bell-LaPadula-like policies
  by the way that a user's permitted accesses are constrained by the history of
  his previous accesses.</p>
<p>We have shown that the formal representation of the policy correctly permits
  a market analyst to talk to any corporation which does not create a conflict of
  interest with previous assignments. Failure to obey this policy would be
  considered, at best, unprofessional and potentially criminally fraudulent. In
  some countries, showing that the policy has been enforced is sufficient to
  provide legal protection against charges of <i>insider dealing</i>.</p>
<p>Thus this is a real commercial policy which can be formally modelled, but not
  using a Bell-LaPadula based approach. However, the Clark and Wilson's access
  control observations are not at variance with either the policy or its model as
  developed within this paper.<br />
                <br />
</p>
<p class="H2">REFERENCES</p>
<p>[1] CLARK, D.R., and WILSON, D.R.<br />
  &quot;A Comparison of Commercial and Military Computer Security Policies&quot;
  IEEE Symposium on Security and Privacy, Oakland, April 1987.</p>
<p>[2] WIPCIS<br />
  &quot;Report of the Invitational Workshop on Integrity Policy in Computer
  Information Systems (WIPCIS)&quot; Published by the NIST (formerly the NBS),
  April 1988.</p>
<p>[3] BREWER, D.F.C.<br />
  &quot;The Corporate Implications of Commercial Security Policies&quot;,
  Proceedings of Corporate Computer Security 89, PLF Communications, London,
  February 1989.</p>
<p>[4] BELL, D.E, and LaPADULA, L.J.<br />
  &quot;Secure Computer Systems: Unified Exposition and Multics
  Interpretation&quot; ESD-TR-75-306, MTR 2997 Rev. 1, The MITRE Corporation,
  March 1976.</p>
<p>[5] McLEAN, J.<br />
  &quot;The Algebra of Security&quot; IEEE Symposium on Security and Privacy,
  Oakland, April 1988.</p>
<p>[6] Her Majesty's Stationery Office, London, Securities and Investment Board
  Rules, Chapter III Part 5:08.</p>
<p>[7] Her Majesty's Stationery Office, London, Financial Services Act 1986,
  S48(2)(h), 1986.</p>
<p class="H1"><a name="ANNEX A" id="ANNEX A">ANNEX A</a>- FORMAL MODEL</p>
<p class="H2">Basic model</p>
<p>Let <i>S</i> be a set of subjects, <i>O</i> be a set of objects and <i>L</i> a set of security labels (<i>x, y</i>). One such label is associated with each
  object. We introduce functions <i><b>X</b></i>(<i>o</i>) and <i><b>Y</b></i>(<i>o</i>)
  which determine respectively the <i>x</i> and <i>y</i> components of this
  security label for a given object <i>o</i>.</p>
<p>We will refer to the <i>x</i> as conflict of interest classes, the <i>y</i> as company datasets and introduce the notation <i>x</i><sub>j</sub>, <i>y</i><sub>j</sub> to mean <i><b>X</b></i>(<i>o</i><sub>j</sub>) and <b><i>Y</i></b>(<i>o</i><sub>j</sub>)
  respectively. Thus for some object <i>o</i><sub>j</sub>, <i>x</i><sub>j</sub> is
  its conflict of interest class and <i>y</i><sub>j</sub> is its company dataset.</p>
<p><b><u>Axiom 1</u></b>:</p>
<p align="center"><i>y</i><sub>1</sub> = <i>y</i><sub>2</sub> ---&gt; <i>x</i><sub>1</sub> = <i>x</i><sub>2</sub></p>
<p>in other words, if any two objects <i>o</i><sub>1</sub> and <i>o</i><sub>2</sub> belong to the same company dataset then they also belong to the same conflict of
  interest class.</p>
<p><u><b>Corollary 1</b></u>:</p>
<p align="center"><i>x</i><sub>1</sub> &lt;&gt; <i>x</i><sub>2</sub> ---&gt; <i>y</i><sub>1</sub> &lt;&gt; <i>y</i><sub>2</sub></p>
<p>in other words, if any two objects <i>o</i><sub>1</sub> and <i>o</i><sub>2</sub> belong to different conflict of interest classes then they must belong to
  different company datasets.</p>
<p align="left"><u><b>Proof</b></u>:</p>
<p align="center"><br />
    <i>y</i><sub>1</sub> = <i>y</i><sub>2</sub> ---&gt; <i>x</i><sub>1</sub> = <i>x</i><sub>2</sub> (A1)</p>
<p align="center"><b><u>not</u></b> (<i>y</i><sub>1</sub> = <i>y</i><sub>2</sub>) <u><b>or</b></u> (<i>x</i><sub>1</sub> = <i>x</i><sub>2</sub>)</p>
<p align="center">(<i>y</i><sub>1</sub> &lt;&gt; <i>y</i><sub>2</sub>) <u><b>or
  not</b></u> (<i>x</i><sub>1</sub> &lt;&gt; <i>x</i><sub>2</sub>)</p>
<p align="center"><i>x</i><sub>1</sub> &lt;&gt; <i>x</i><sub>2</sub> ---&gt; <i>y</i><sub>1</sub> &lt;&gt; <i>y</i><sub>2</sub></p>
<p><u><b>Definition 1</b></u>:</p>
<p><u><b>N</b></u>, a boolean matrix with elements <u><b>N</b></u>(v, c)
  corresponding to the members of <i>S</i>x<i>O</i> which take the value true if
  subject <i>s</i><sub>v</sub> has, or has had, access to object <i>o</i><sub>c</sub> or the value false if <i>s</i><sub>v</sub> has not had access to object <i>o</i><sub>c</sub>.
  Once some request R(u, r) by subject <i>s</i><sub>u</sub> to access some new
  object <i>o</i><sub>r</sub> has been granted then <u><b>N</b></u>(u, r) must be
  set true to reflect the fact that access has now been granted. Thus, without
  loss of generality, any request R(u, r) causes a state transition whereby <u><b>N</b></u> is replaced by some new <u><b>N</b></u>, <u><b>N</b></u>'.</p>
<p><u><b>Axiom 2</b></u>:</p>
<p>Access to any object <i>o</i><sub>r</sub> by any subject <i>s</i><sub>u</sub> is granted if and only if for all <u><b>N</b></u>(u, c) = true (i.e. by D1 <i>s</i><sub>u</sub> has had access to <i>o</i><sub>c</sub>)</p>
<p align="center">((<i>x</i><sub>c</sub> &lt;&gt; <i>x</i><sub>r</sub>) or (<i>y</i><sub>c</sub> = <i>y</i><sub>r</sub>)).<br />
</p>
<p><u><b>Axiom 3</b></u>:</p>
<p><u><b>N</b></u>(v, c) = false, <b><u>for</u> <u>all</u></b> (v, c) represents
  an initially secure state.</p>
<p><u><b>Axiom 4</b></u>:</p>
<p>If <u><b>N</b></u>(u, c) is everywhere false for some <i>s</i><sub>u</sub> then any request R(u, r) is granted.</p>
<p><u><b>Theorem 1</b></u>:</p>
<p>Once a subject has accessed an object the only other objects accessible by
  that subject lie within the same company dataset or within a different conflict
  of interest class.</p>
<p><u><b>Proof</b></u>:</p>
<p>If this proposition is untrue then it is possible for some subject <i>s</i><sub>u</sub> to have access to two objects, <i>o</i><sub>a</sub> and <i>o</i><sub>b</sub>,
  which belong to the same conflict of interest class but different company
  datasets, i.e.</p>
<p align="center"><u><b>N</b></u>(u, a) = true <b><u>and</u></b> <u><b>N</b></u>(u,
  b) = true <u><b>and</b></u> <i>x</i><sub>a</sub> = <i>x</i><sub>b</sub> and <i>y</i><sub>a</sub> &lt;&gt; <i>y</i><sub>b</sub></p>
<p>Let us assume without loss of generality that access to <i>o</i><sub>a</sub> was granted first. Then, when access to <i>o</i><sub>b</sub> was granted <u><b>N</b></u>(u,
  a) was already true and thus by A2 (<i>x</i><sub>a</sub> &lt;&gt; <i>x</i><sub>b</sub>) <u><b>or</b></u> (<i>y</i><sub>a</sub> = <i>y</i><sub>b</sub>). Then for our two
  objects <i>o</i><sub>a</sub> and <i>o</i><sub>b</sub> to exist:</p>
<p align="center">(<i>x</i><sub>a</sub> &lt;&gt; <i>x</i><sub>b</sub> <u><b>or</b></u> <i>y</i><sub>a</sub> = <i>y</i><sub>b</sub>) <u><b>and</b></u> (<i>x</i><sub>a</sub> = <i>x</i><sub>b</sub> <u><b>and</b></u> <i>y</i><sub>a</sub> &lt;&gt; <i>y</i><sub>b</sub>)</p>
<p align="center">(<i>x</i><sub>a</sub> &lt;&gt; <i>x</i><sub>b</sub> <u><b>and</b></u> <i>x</i><sub>a</sub> = <i>x</i><sub>b</sub> <u><b>and</b></u> <i>y</i><sub>a</sub> &lt;&gt; <i>y</i><sub>b</sub>) <u><b>or</b></u> (<i>y</i><sub>a</sub> = <i>y</i><sub>b</sub> <u><b>and</b></u> <i>x</i><sub>a</sub> = <i>x</i><sub>b</sub> <u><b>and</b></u> <i>y</i><sub>a</sub> &lt;&gt; <i>y</i><sub>b</sub>)</p>
<p>which is always false.</p>
<p><u><b>Theorem 2</b></u>:</p>
<p>A subject can at most have access to one company dataset in each conflict of
  interest class.<br />
</p>
<p><u><b>Proof</b></u>:</p>
<p>A subject need not have access to any dataset since by A3, <u><b>N</b></u> is
  initially everywhere false. Suppose <i>s</i><sub>u</sub> then requests access to <i>o</i><sub>p</sub>. This request succeeds by A4 and our subject has access to
  one object within one company dataset.</p>
<p>By T1 the only objects then accessible to <i>s</i><sub>u</sub> lie within the
  same company dataset or within a different conflict of interest class, <i>x</i><sub>q</sub> &lt;&gt; <i>x</i><sub>p</sub>. Hence at most only one company dataset within any
  particular conflict of interest class is accessible to a single subject.</p>
<p><u><b>Theorem 3</b></u>:</p>
<p>If for some conflict of interest class X there are X<sub>Y</sub> company
  datasets then the minimum number of subjects which will allow every object to be
  accessed by at least one subject is X<sub>Y</sub>.</p>
<p><u><b>Proof</b></u>:</p>
<p>Suppose there are N subjects and for some conflict of interest class, X,
  there are X<sub>Y</sub> company datasets. Let all of these subjects have access
  to the same company dataset, i.e. N subjects have access to company dataset (X,
  1); and, by T2, no subject has access to company datasets (X, 2)...(X, X<sub>Y</sub>).</p>
<p>We can access one of these, say (X, 2), by reallocating one of the subjects
  with access to (X, 1) to (X, 2), i.e. N - 1 subjects with access to (X, 1), one
  with access to (X, 2) and X<sub>Y</sub> - 2 inaccessible datasets.</p>
<p>By induction, after n similar reallocations we have N - n subjects with
  access to (X, 1), one each for (X, 2)...(X, n + 1) and X<sub>Y</sub> - (n + 1)
  inaccessible datasets. In order that all data sets are accessible we require X<sub>Y</sub> = (n + 1) provided, of course, that the number of subjects with access to (X, 1)
  is at least one, i.e. N - n &gt; 0. Hence we require the smallest value of N
  such that:</p>
<p align="center">X<sub>Y</sub> = (n + 1) <u><b>and</b></u> N - n &gt; 0</p>
<p align="center">N - X<sub>Y</sub> + 1 &gt; 0</p>
<p>which has a minimum when</p>
<p align="center">N - X<sub>Y</sub> + 1 = 1</p>
<p>i.e. when N = X<sub>Y</sub>.</p>
<p class="H2">Sanitized information</p>
<p><u><b>Definition 2</b></u>:</p>
<p>For any object <i>o</i><sub>s</sub>,</p>
<p><i>y</i>s = <i>y</i><sub>o</sub> implies that <i>o</i><sub>s</sub> contains
  sanitized information</p>
<p><i>y</i><sub>s</sub> &lt;&gt; <i>y</i><sub>o</sub> implies that <i>o</i><sub>s</sub> contains unsanitized information</p>
<p align="left"><u><b>Axiom 5</b></u>:</p>
<p align="center"><i>y</i><sub>o</sub> &lt;---&gt; <i>x</i><sub>o</sub></p>
<p>In other words, if an object bears the security label <i>y</i><sub>o</sub> then it must also bear the label <i>x</i><sub>o</sub> and vice versa. T2 tells
  us that all subjects can access this company dataset.</p>
<p><u><b>Axiom 6</b></u>:</p>
<p>Write access to any object <i>o</i><sub>b</sub> by any subject <i>s</i><sub>u</sub> is permitted if and only if <u><b>N</b></u>'(u, b) = true and there does not
  exist any object <i>o</i><sub>a</sub> (<u><b>N</b></u>'(u, a) = true) which can
  be read by <i>s</i><sub>u</sub> for which:</p>
<p align="center"><i>y</i><sub>a</sub> &lt;&gt; <i>y</i><sub>b</sub> <u><b>and</b></u> <i>y</i><sub>a</sub> &lt;&gt; <i>y</i><sub>o</sub>.</p>
<p align="left"><br />
    <u><b>Theorem 4</b></u>:</p>
<p>The flow of unsanitized information is confined to its own company dataset;
  sanitized information may however flow freely throughout the system.</p>
<p><u><b>Proof</b></u>:</p>
<p>Let <i>T</i> = {(a, b)|<u><b>N</b></u>'(u, a) = true <u><b>and</b></u> <u><b>N</b></u>'(u,
  b) = true for some <i>s</i><sub>u</sub> in <i>S</i>}. We will interpret (a, b)
  as meaning that information may flow from <i>o</i><sub>a</sub> to <i>o</i><sub>b</sub>.
  The reflexive transitive closure <i>T</i>* then defines all possible information
  flows.</p>
<p>Let <i>B</i> = {(a, b)|(a, b) in <i>O</i>x<i>O</i> <u><b>and</b></u> <i>y</i><sub>a</sub> &lt;&gt; <i>y</i><sub>b</sub> and <i><sub>y</sub></i>a &lt;&gt; <i>y</i><sub>o</sub>}.
  This is the set of all object pairs excluded by A6.</p>
<p>Thus the only possible information flows remaining after the introduction of
  A6 are given by <i>C</i> = <i>T</i>* <u><b>minus</b></u> <i>B</i>:</p>
<p align="center">{(a, b)|<u><b>not</b></u> (<i>y</i><sub>a</sub> &lt;&gt; <i>y</i><sub>b</sub> <u><b>and</b></u> <i>y</i><sub>a</sub> &lt;&gt; <i>y</i><sub>o</sub>)}</p>
<p align="center"><br />
  {(a, b)|<u><b>not</b></u> (<i>y</i><sub>a</sub> &lt;&gt; <i>y</i><sub>b</sub>)
  or <u><b>not</b></u> (<i>y</i><sub>a</sub> &lt;&gt; <i>y</i><sub>o</sub>)}</p>
<p align="center"><br />
  {(a, b)|(<i>y</i><sub>a</sub> = <i>y</i><sub>b</sub>) <u><b>or</b></u> (<i>y</i><sub>a</sub> = <i>y</i><sub>o</sub>)}</p>
<p>Hence information may only flow between objects which belong to the same
  company dataset or originate from the sanitized dataset.</p>
<p class="H2">Extension for Clark and Wilson</p>
<p>We now formally introduce the set <i>P</i>, the set of processes which we may
  interpret as those programs or sub-programs which a user may use to access
  whatever objects that the Chinese Wall policy grants him access to. We let <i>A</i> be a relation of <i>S</i>x<i>P</i>, representing those processes which a user is
  allowed to execute. Thus:</p>
<p><u><b>Axiom 7</b></u>:</p>
<p>A user, <i>s</i><sub>u</sub>, may execute a process, <i>p</i><sub>f</sub>, if
  and only if (u, f) is a member of <i>A</i>.</p>
<p>We augment <i>L</i> to include a third attribute, <i>z</i> (i.e. <i>L</i> =
  {(<i>x, y, z</i>)}, where <i>z</i> is a member of some set <i>Z</i>. <b>Z</b>(<i>o</i><sub>j</sub>)
  is the function which determines the <i>z</i>-component of the security label of
  a given object <i>o</i><sub>j</sub> (<i>z</i><sub>j</sub> for short) and
  introduce <i>PZ</i> to represent the power set of <i>Z</i>. We then associate
  with each and every process, <i>p</i><sub>f</sub>, a member of <i>PZ</i>,
  determined by the function <b>PZ</b>(pf), or <i>pz</i><sub>f</sub> for short. We
  assert that processes can only access objects whose <i>z</i>-attribute is
  included in those of the process, i.e.</p>
<p><u><b>Axiom 8</b></u>:</p>
<p>A process <i>p</i><sub>f</sub> may only access an object <i>o</i><sub>r</sub> if&nbsp;</p>
<p align="center"><i>z</i><sub>r</sub> <b><u>subset</u> <u>of</u></b> <i>pz</i><sub>f</sub>.</p>
<p>The access rules are now governed by A2-A4 and A6-A8. In particular an
  initially secure state exists when no user has ever accessed any object (A3) and
  the first ever access by any user to any object is entirely unrestrained (A4).</p>
<p>Users may, however, only access that object (say <i>s</i><sub>u</sub> and <i>o</i><sub>r</sub> respectively) via some process <i>p</i><sub>f</sub> where (u, f) in <i>A</i> and
  for which <i>z</i><sub>r</sub> subset of <i>pz</i><sub>f</sub> (A7, A8).</p>
<p>Users may then access some other object or via pf if and only if for all n(u,
  c) = true:</p>
<p align="center">((u, f) in <i>A</i>)<b> <u>and</u> </b>(<i>z</i><sub>r</sub> <u><b>subset</b></u> <u><b>of</b></u> <i>pz</i><sub>f</sub>) <b><u>and</u> </b>((<i>x</i><sub>c</sub> &lt;&gt; <i>x</i><sub>r</sub>) <u><b>or</b></u> (<i>y</i>c = <i>y</i><sub>r</sub>))<br />
</p>
<p align="right">(A2, A7, A8)</p>
<p>and, finally users may only write information to an object <i>o</i><sub>b</sub> provided that access is not prohibited by any of the above rules and that there
  does not exist any object <i>o</i><sub>a</sub> which can be read for which:</p>
<p align="center"><i>y</i><sub>a</sub> &lt;&gt; <i>y</i><sub>b</sub> and <i>y</i><sub>a</sub> &lt;&gt; <i>y</i><sub>o</sub><br />
</p>
<p align="right">(A6).</p>
<!-- InstanceEndEditable --></td>
    <td width="16"></td>
    <td width="8" bgcolor="#005A52"></td>
  </tr>
  <tr>
    <td width="8" bgcolor="#005A52"></td>
    <td width="16"></td>
    <td colspan="2">&nbsp;</td>
    <td width="16"></td>
    <td width="8" bgcolor="#005A52"></td>
  </tr>
  <tr>
    <td width="8" bgcolor="#005A52"></td>
    <td width="16"></td>
    <td colspan="2"><p class="footer">This website does not use cookies.</p></td>
    <td width="16"></td>
    <td width="8" bgcolor="#005A52"></td>
  </tr>
  <tr><td height="3" colspan="6" bgcolor="#005A52"></td>
  </tr>
  <tr>
    <td  colspan="3" class="footer">&copy; <u title="Reproduction of part or all of the contents of any of these pages is prohibited except to the following extent:

These pages may be downloaded onto a hard disk or printed for your personal use provided that you include this copyright notice on each copy and that you make no alterations to any of the pages and do not use any of the pages in any other work or publication in whatever medium stored.

No part of the Gamma Secure Systems Limited pages may be distributed or copied for any commercial purpose.">Gamma Secure Systems Limited</u>, <!-- InstanceBeginEditable name="CopyRightDates" -->2013<!-- InstanceEndEditable --></td>
    <td  colspan="3"><div align="right" class="footer">Page last updated: <!-- InstanceBeginEditable name="DateofPageUpdate" --><!-- #BeginDate format:Sw1 -->18 January, 2013<!-- #EndDate --><!-- InstanceEndEditable --></div></td>
  </tr>
  </table>

</body>
<!-- InstanceEnd --></html>
